RAG (Retrieval-Augmented Generation)

RAG: 问题 -> LLM + 知识 -> 回复

微调: LLM + 知识 -> (问题 -> LLM -> 回复)

场景:

1. 动态数据(比如企业中的动态数据): RAG
2. 模型能力定制(比如某领域的年报/口吻): 微调
3. 幻觉: RAG > 微调
4. 可解释性: RAG >= 微调
5. 低延迟场景: 微调
6. 智能设备(硬件受限): 微调

RAG 可以降低模型的幻觉 (AI Hallucinations)

RAG 流程过于复杂, 包含检索, 精排, 比较耗时

# 幻觉 AI Hallucinations

幻觉是由于模型所生成的输出没有任何已知事实的支持 (训练数据错误或不足), 或者模型本身的偏见导致, 而且 LLM 一般训练后不会回应 "我不知道", 所以会导致回复一些明显错误的答案并且一直继续下去

主要因素:

1. 训练数据有偏见或训练数据不足
    - AI 模型的好坏取决于训练所使用的数据, 如果训练数据有偏见, 不完整, 不足, 模型会基于其对所访问数据的有限理解而产生幻觉. 在使用开放的互联网数据训练大型语言模型的情况下, 因为互联网中有偏见和错误的信息泛滥, 所以会出现
2. 过度拟合
    - 过拟合会开始生成对训练数据过于具体的输出, 不能很好地推广到新数据
3. 上下文理解缺乏
    - 缺乏上下文理解, 导致输出荒谬
4. 领域知识有限
    - 类似互联网爬虫数据集导致, 在特定领域任务, 缺乏生成相关输出所需的知识或背景, 导致对不同语言的理解有限, 或缺乏文化背景, 历史, 细微差别, 无法正确的将概念串在一起
