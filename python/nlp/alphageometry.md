# alphageometry2

2025/02/05

@see: https://arxiv.org/abs/2502.03544

@see: https://github.com/google-deepmind/alphageometry

We present AlphaGeometry2, a significantly improved version of AlphaGeometry introduced in Trinh et al. (2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle harder problems involving movements of objects, and problems containing linear equations of angles, ratios, and distances.(æ¶‰åŠç‰©ä½“ç§»åŠ¨ã€è§’åº¦çº¿æ€§æ–¹ç¨‹ã€æ¯”ä¾‹å’Œè·ç¦»çš„é—®é¢˜) This, together with other additions, has markedly improved the coverage rate of the AlphaGeometry language on International Math Olympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process of AlphaGeometry2 has also been greatly improved through the use of Gemini architecture for better language modeling, and a novel knowledge-sharing mechanism that combines multiple search trees.(é€šè¿‡ä½¿ç”¨ Gemini æ¶æ„å®ç°äº†æ›´å¥½çš„è¯­è¨€å»ºæ¨¡ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„çŸ¥è¯†å…±äº«æœºåˆ¶ï¼Œå°†å¤šä¸ªæœç´¢æ ‘ç»“åˆèµ·æ¥) Together with further enhancements to the symbolic engine and synthetic data generation,(å¯¹ç¬¦å·å¼•æ“å’Œåˆæˆæ•°æ®ç”Ÿæˆè¿›è¡Œäº†è¿›ä¸€æ­¥çš„å¢å¼º) we have significantly boosted the overall solving rate of AlphaGeometry2 to 84% for all geometry problems over the last 25 years, compared to 54% previously. AlphaGeometry2 was also part of the system that achieved silver-medal standard at IMO 2024 https://dpmd.ai/imo-silver. Last but not least, we report progress towards using AlphaGeometry2 as a part of a fully automated system that reliably solves geometry problems directly from natural language input.

AlphaGeometry2 Key Improvements:

â€¢ Expanded Domain Language: Covering locus-type theorems, linear equations, and nonconstructive problem statements. (æ‰©å±•çš„é¢†åŸŸè¯­è¨€ï¼šæ¶µç›–äº†è½¨è¿¹å®šç†ã€çº¿æ€§æ–¹ç¨‹å’Œéæ„é€ æ€§é—®é¢˜é™ˆè¿°)

â€¢ Stronger and faster Symbolic Engine: Optimized rule set, added handling of double points, and a faster implementation in C++.(æ›´å¼ºå¤§å’Œæ›´å¿«çš„ç¬¦å·å¼•æ“ï¼šä¼˜åŒ–äº†è§„åˆ™é›†ï¼Œå¢åŠ äº†å¯¹åŒç‚¹çš„å¤„ç†ï¼Œå¹¶åœ¨ C++ä¸­å®ç°äº†æ›´å¿«çš„å®ç°)

â€¢ Advanced Novel Search Algorithm: Utilizing multiple search trees with knowledge sharing.(é«˜çº§æ–°é¢–æœç´¢ç®—æ³•ï¼šåˆ©ç”¨å¤šæ£µæœç´¢æ ‘å¹¶é‡‡ç”¨çŸ¥è¯†å…±äº«æœºåˆ¶)

â€¢ Enhanced Language Model: Leveraging the Gemini architecture, trained on a larger and more diverse dataset.(å¢å¼ºçš„è¯­è¨€æ¨¡å‹ï¼šåˆ©ç”¨ Gemini æ¶æ„ï¼Œè®­ç»ƒäºæ›´å¤§ä¸”æ›´å¤šæ ·åŒ–çš„æ•°æ®é›†)

## beam search

è¿™é‡Œè¦å…ˆå­¦ beam search åŸç†:

ç”Ÿæˆå¼ä»»åŠ¡ç›¸æ¯”æ™®é€šçš„åˆ†ç±»ã€tagging ç­‰ NLP ä»»åŠ¡ä¼šå¤æ‚ä¸å°‘ã€‚åœ¨ç”Ÿæˆçš„æ—¶å€™ï¼Œæ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªæ—¶é—´æ­¥ä¸€ä¸ªæ—¶é—´æ­¥ä¾æ¬¡è·å¾—çš„ï¼Œè€Œä¸”å‰é¢æ—¶é—´æ­¥çš„ç»“æœè¿˜ä¼šå½±å“åé¢æ—¶é—´æ­¥çš„ç»“æœã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œæ¨¡å‹ç»™å‡ºçš„éƒ½æ˜¯åŸºäºå†å²ç”Ÿæˆç»“æœçš„æ¡ä»¶æ¦‚ç‡ã€‚ä¸ºäº†ç”Ÿæˆå®Œæ•´çš„å¥å­ï¼Œéœ€è¦ä¸€ä¸ªç§°ä¸ºè§£ç çš„é¢å¤–åŠ¨ä½œæ¥èåˆæ¨¡å‹å¤šä¸ªæ—¶é—´æ­¥çš„è¾“å‡ºï¼Œè€Œä¸”ä½¿å¾—æœ€ç»ˆå¾—åˆ°çš„åºåˆ—çš„æ¯ä¸€æ­¥æ¡ä»¶æ¦‚ç‡è¿ä¹˜èµ·æ¥æœ€å¤§

åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæ¯ä¸€ä¸ªæ—¶é—´æ­¥å¯èƒ½çš„è¾“å‡ºç§ç±»ç§°ä¸ºå­—å…¸å¤§å°(vocabulary sizeï¼Œæˆ‘ä»¬ç”¨ V è¡¨ç¤º)ï¼Œè¿›è¡Œ T æ­¥éšæœºçš„ç”Ÿæˆå¯èƒ½è·å¾—çš„ç»“æœæ€»å…±æœ‰ V^T ç§ã€‚æ‹¿ä¸­æ–‡æ–‡æœ¬ç”Ÿæˆæ¥è¯´ï¼ŒV çš„å€¼å¤§çº¦æ˜¯ 5000-6000ï¼Œå³å¸¸ç”¨æ±‰å­—çš„ä¸ªæ•°ã€‚åœ¨å¦‚æ­¤å¤§çš„åŸºæ•°ä¸‹ï¼Œéå†æ•´ä¸ªç”Ÿæˆç©ºé—´æ˜¯ä¸ç°å®çš„

bean search æ€è·¯ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯ç¨å¾®æ”¾å®½ä¸€äº›è€ƒå¯Ÿçš„èŒƒå›´ã€‚åœ¨æ¯ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œä¸å†åªä¿ç•™å½“å‰åˆ†æ•°æœ€é«˜çš„ 1 ä¸ªè¾“å‡ºï¼Œè€Œæ˜¯ä¿ç•™ num_beams ä¸ªã€‚å½“ num_beams=1 æ—¶é›†æŸæœç´¢å°±é€€åŒ–æˆäº†è´ªå¿ƒæœç´¢

Beam Search çš„åŸç†è™½ç„¶ç®€å•ï¼Œä½†å®é™…å®ç°çš„æ—¶å€™å´æœ‰å¾ˆå¤šç»†èŠ‚è¦è€ƒè™‘ã€‚ä¸‹é¢è¦è§£æè¿™ä¸ªå®ç°å‡ºè‡ªäº NLP ç•Œè‘—å Python åŒ… Transformers[1]ï¼Œæˆ‘ä¸ºäº†è¯´æ˜æ–¹ä¾¿åšäº†ä¸€äº›æ”¹åŠ¨ã€‚

ä¸€ä¸ªæ­£ç¡®ä¸”é«˜æ•ˆçš„ç®—æ³•éœ€è¦å¤„ç†çš„é—®é¢˜å¤§æ¦‚æœ‰ä¸¤ä¸ªï¼š

å……åˆ†åˆ©ç”¨ç¡¬ä»¶ï¼Œå¯ä»¥å¤„ç†æ‰¹é‡æ•°æ®ï¼Œä¸”å°½é‡ä½¿ç”¨å¹¶è¡Œè®¡ç®—å°‘ç”¨å¾ªç¯
å¤„ç†å¥½é•¿çŸ­ä¸åŒçš„ç”Ÿæˆç»“æœ

@see: https://www.cnblogs.com/nickchen121/p/15499576.html

Beam Search æ˜¯ä¸€ç§å¯å‘å¼æœç´¢ç®—æ³•ï¼Œä¸»è¦ç”¨äºåœ¨åºåˆ—ç”Ÿæˆä»»åŠ¡ä¸­å¯»æ‰¾æœ€ä¼˜æˆ–è¿‘ä¼¼æœ€ä¼˜çš„è¾“å‡ºåºåˆ—ã€‚å®ƒæ˜¯å¯¹è´ªå¿ƒæœç´¢ï¼ˆGreedy Searchï¼‰çš„æ”¹è¿›ï¼Œé€šè¿‡åœ¨æ¯ä¸€æ­¥ä¿æŒ k ä¸ªæœ€ä½³å€™é€‰é¡¹æ¥å¹³è¡¡æœç´¢ç©ºé—´å’Œè®¡ç®—æ•ˆç‡

ä¸ºä»€ä¹ˆéœ€è¦ Beam Searchï¼Ÿ

1. è§£å†³è´ªå¿ƒæœç´¢çš„å±€é™æ€§ï¼š
2. è´ªå¿ƒæœç´¢æ¯æ­¥åªé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ä¸€ä¸ªè¯
3. å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜è§£
4. å¯èƒ½é”™è¿‡å…¨å±€æœ€ä¼˜è§£
5. è®¡ç®—æ•ˆç‡è€ƒè™‘ï¼š
6. ç©·ä¸¾æ‰€æœ‰å¯èƒ½ï¼ˆæš´åŠ›æœç´¢ï¼‰è®¡ç®—é‡è¿‡å¤§
7. Beam Search æä¾›äº†ä¸€ä¸ªæŠ˜ä¸­æ–¹æ¡ˆ

@see: https://mdnice.com/writing/3532584cdd404e5893495914b1b2a8a6

alphageometry2 çš„ bean_search.py:

1. å¸¸é‡å®šä¹‰:

-   `NEG_INF`: ç”¨äºå±è”½çš„â€œæœ‰æ•ˆè´Ÿæ— ç©·å¤§â€å¸¸é‡ï¼Œå€¼ä¸º-1e7ã€‚
-   `BEAM_SEARCH_DEFAULT_ALPHA`: Beam Search çš„é»˜è®¤ç¼©æ”¾å‚æ•°ï¼Œç”¨äºè®¡ç®—é•¿åº¦æƒ©ç½šã€‚
-   `MAX_DECODE_LEN`: æœ€å¤§è§£ç é•¿åº¦ï¼Œè¿™é‡Œæ˜¯ 32ã€‚
-   `BREVITY_LEN_BIAS_NUMERATOR` å’Œ `BREVITY_LEN_BIAS_DENOMINATOR`: è®¡ç®—é•¿åº¦æƒ©ç½šæ—¶çš„å‚æ•°ï¼Œç”¨äºæƒ©ç½šçŸ­åºåˆ—ã€‚

2. Brevity Penalty å‡½æ•°:

-   è¯¥å‡½æ•°ç”¨äºè®¡ç®—é•¿åº¦æƒ©ç½šï¼Œä½¿ Beam Search æ›´å€¾å‘äºç”Ÿæˆè¾ƒé•¿çš„åºåˆ—ã€‚è¾“å…¥ä¸ºç¼©æ”¾å‚æ•°`alpha`å’Œåºåˆ—é•¿åº¦`length`ï¼Œè¾“å‡ºä¸ºä¸€ä¸ª JAX æ ‡é‡ï¼Œä»£è¡¨é•¿åº¦æƒ©ç½šåˆ†æ•°ã€‚
-   è®¡ç®—å…¬å¼ä¸ºï¼š$\left(\frac{BREVITY\_LEN\_BIAS\_NUMERATOR + $length$}{BREVITY_LEN_BIAS_DENOMINATOR}\right)^\alpha$

```python
def brevity_penalty(alpha: float, length: int):
  """Brevity penalty function for beam search penalizing short sequences.

  Args:
    alpha: float: brevity-penalty scaling parameter.
    length: int: length of considered sequence.

  Returns:
    Brevity penalty score as jax scalar.
  """
  return jnp.power(
      ((BREVITY_LEN_BIAS_NUMERATOR + length) / BREVITY_LEN_BIAS_DENOMINATOR),
      alpha,
  )
```

3. Beam Search å·¥å…·å‡½æ•°:

-   add_beam_dim: å‘éæ ‡é‡æ•°ç»„æ·»åŠ ä¸€ä¸ªæ–°çš„æŸç»´åº¦ï¼Œå¹¶æ ¹æ®æŒ‡å®šçš„ beam_size è¿›è¡Œå¤åˆ¶

    ```py
    def add_beam_dim(x: jnp.ndarray, beam_size: int) -> jnp.ndarray:
    """Creates new beam dimension in non-scalar array and tiles into it."""
    if x.ndim == 0:  # ignore scalars (e.g. cache index)
        return x
    x = jnp.expand_dims(x, axis=1)
    tile_dims = [1] * x.ndim
    tile_dims[1] = beam_size
    return jnp.tile(x, tile_dims)
    ```

-   add_beam_dim_cache: å‘æ³¨æ„åŠ›ç¼“å­˜çš„ keys å’Œ vals æ·»åŠ æŸç»´åº¦å¹¶è¿›è¡Œå¤åˆ¶ã€‚

    ```py
    def add_beam_dim_cache(
        cache: tuple[dict[str, jnp.ndarray], ...], beam_size: int
    ) -> tuple[dict[str, jnp.ndarray], ...]:
    """Creates new beam dimension in non-scalar array and tiles into it."""
    new_cache = []

    for layer in cache:
        new_layer = {}
        for key, x in layer.items():
        if key in ['keys', 'vals']:
            x = add_beam_dim(x, beam_size)
        new_layer[key] = x
        new_cache.append(new_layer)

    return tuple(new_cache)

    ```

-   flatten_beam_dim: å°†éæ ‡é‡æ•°ç»„çš„å‰ä¸¤ä¸ªç»´åº¦å±•å¹³

    ```py
    def flatten_beam_dim(x):
    """Flattens the first two dimensions of a non-scalar array."""
    if x.ndim < 2:  # ignore scalars (e.g. cache index)
        return x
    return x.reshape((x.shape[0] * x.shape[1],) + x.shape[2:])

    ```

-   unflatten_beam_dim: å°†å±•å¹³çš„å‰ä¸¤ä¸ªç»´åº¦è¿˜åŸä¸ºæŸç»´åº¦

    ```py
    def unflatten_beam_dim(x, batch_size, beam_size):
    """Unflattens the first, flat batch*beam dimension of a non-scalar array."""
    if x.ndim == 0:  # ignore scalars (e.g. cache index)
        return x
    assert batch_size * beam_size == x.shape[0]
    return x.reshape((batch_size, beam_size) + x.shape[1:])

    ```

-   flat_batch_beam_expand: æ ¹æ® beam_size å¢åŠ æ¯ä¸ªæ‰¹æ¬¡é¡¹çš„æŸç»´åº¦

    ```py
    def flat_batch_beam_expand(x, beam_size):
    """Expands the each batch item by beam_size in batch_dimension."""
    return flatten_beam_dim(add_beam_dim(x, beam_size))
    ```

-   gather_beams: æ ¹æ®æŒ‡å®šçš„æŸç´¢å¼•æ”¶é›†æŸåˆ‡ç‰‡ï¼Œå¹¶å½¢æˆæ–°çš„æŸæ•°ç»„

    ```py
    def gather_beams(nested, beam_indices, batch_size, new_beam_size):
        """Gathers the beam slices indexed by beam_indices into new beam array.

        Args:
            nested: pytree of arrays or scalars (the latter ignored).
            beam_indices: array of beam_indices
            batch_size: int: size of batch.
            new_beam_size: int: size of _new_ beam dimension.

        Returns:
            New pytree with new beam arrays.
            [batch_size, old_beam_size, ...] --> [batch_size, new_beam_size, ...]
        """
        batch_indices = jnp.reshape(
            jnp.arange(batch_size * new_beam_size) // new_beam_size,
            (batch_size, new_beam_size),
        )

    def gather_fn(x):
        if x.ndim == 0:  # ignore scalars (e.g. cache index)
        return x
        else:
        return x[batch_indices, beam_indices]

    return jax.tree_util.tree_map(gather_fn, nested)

    ```

-   gather_topk_beams: æ ¹æ®åˆ†æ•°æˆ–å¯¹æ•°æ¦‚ç‡æ•°ç»„æ”¶é›†å‰ k ä¸ªæŸåˆ‡ç‰‡ï¼Œå¹¶å½¢æˆæ–°çš„æŸæ•°ç»„

    ```py
    def gather_topk_beams(nested, score_or_log_prob, batch_size, new_beam_size):
    """Gathers the top-k beam slices given by score_or_log_prob array.

    Args:
        nested: pytree of arrays or scalars (the latter ignored).
        score_or_log_prob: [batch_size, old_beam_size] array of values to sort by
        for top-k selection of beam slices.
        batch_size: int: size of batch.
        new_beam_size: int: size of _new_ top-k selected beam dimension

    Returns:
        New pytree with new beam arrays containing top k new_beam_size slices.
        [batch_size, old_beam_size, ...] --> [batch_size, new_beam_size, ...]
    """
    _, topk_indices = lax.top_k(score_or_log_prob, k=new_beam_size)
    topk_indices = jnp.flip(topk_indices, axis=1)
    return gather_beams(nested, topk_indices, batch_size, new_beam_size)

    ```

-   apply_on_cache: åªå¯¹ç¼“å­˜ä¸­çš„ç‰¹å®šé”®ï¼ˆkeysã€valuesã€current_indexã€relative_position_biasï¼‰åº”ç”¨æŒ‡å®šçš„å‡½æ•°

    ```py
    def apply_on_cache(fn, cache, *args, **kwargs):
    """Apply fn(val) only when key is 'keys' or 'val'."""
    new_cache = []
    for layer in cache:
        new_layer = {}
        for key, val in layer.items():
        if key in ['keys', 'values', 'current_index', 'relative_position_bias']:
            val = fn(val, *args, **kwargs)
        new_layer[key] = val
        new_cache.append(new_layer)
    return tuple(new_cache)

    ```

æ€»ç»“: å®ƒé¦–å…ˆåˆå§‹åŒ–è§£ç çŠ¶æ€ï¼Œç„¶ååœ¨ä¸€ä¸ªå¾ªç¯ä¸­ä¸æ–­æ‰©å±•å€™é€‰åºåˆ—ï¼Œé€‰æ‹©å‡ºæ¦‚ç‡æœ€é«˜çš„åºåˆ—ç»§ç»­æ‰©å±•ã€‚åŒæ—¶ï¼Œå®ƒè¿˜è€ƒè™‘äº†é•¿åº¦æƒ©ç½šï¼Œä»¥é¿å…ç”Ÿæˆè¿‡çŸ­çš„åºåˆ—ã€‚æ•´ä¸ªè¿‡ç¨‹åˆ©ç”¨äº† JAX åº“çš„é«˜æ•ˆè®¡ç®—èƒ½åŠ›

### beam_search-py

```py
# Copyright 2023 DeepMind Technologies Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

"""Fast decoding routines for inference from a trained model.

Modified https://github.com/google/flax/blob/main/examples/wmt/decode.py
to acommodate

(a) continued decoding from a previous beam cache.
(b) init with with a single beam and then expand into beam_size beams.
"""

from typing import Any

import flax
import jax
from jax import lax
import jax.numpy as jnp
import numpy as np


# Constants
# "Effective negative infinity" constant for masking in beam search.
NEG_INF = np.array(-1.0e7)

# Beam search parameters
BEAM_SEARCH_DEFAULT_ALPHA = 0.6
MAX_DECODE_LEN = 32

# Brevity penalty parameters
BREVITY_LEN_BIAS_NUMERATOR = 5.0
BREVITY_LEN_BIAS_DENOMINATOR = 6.0


def brevity_penalty(alpha: float, length: int):
  """Brevity penalty function for beam search penalizing short sequences.

  Args:
    alpha: float: brevity-penalty scaling parameter.
    length: int: length of considered sequence.

  Returns:
    Brevity penalty score as jax scalar.
  """
  return jnp.power(
      ((BREVITY_LEN_BIAS_NUMERATOR + length) / BREVITY_LEN_BIAS_DENOMINATOR),
      alpha,
  )


# Beam handling utility functions:


def add_beam_dim(x: jnp.ndarray, beam_size: int) -> jnp.ndarray:
  """Creates new beam dimension in non-scalar array and tiles into it."""
  if x.ndim == 0:  # ignore scalars (e.g. cache index)
    return x
  x = jnp.expand_dims(x, axis=1)
  tile_dims = [1] * x.ndim
  tile_dims[1] = beam_size
  return jnp.tile(x, tile_dims)


def add_beam_dim_cache(
    cache: tuple[dict[str, jnp.ndarray], ...], beam_size: int
) -> tuple[dict[str, jnp.ndarray], ...]:
  """Creates new beam dimension in non-scalar array and tiles into it."""
  new_cache = []

  for layer in cache:
    new_layer = {}
    for key, x in layer.items():
      if key in ['keys', 'vals']:
        x = add_beam_dim(x, beam_size)
      new_layer[key] = x
    new_cache.append(new_layer)

  return tuple(new_cache)


def flatten_beam_dim(x):
  """Flattens the first two dimensions of a non-scalar array."""
  if x.ndim < 2:  # ignore scalars (e.g. cache index)
    return x
  return x.reshape((x.shape[0] * x.shape[1],) + x.shape[2:])


def unflatten_beam_dim(x, batch_size, beam_size):
  """Unflattens the first, flat batch*beam dimension of a non-scalar array."""
  if x.ndim == 0:  # ignore scalars (e.g. cache index)
    return x
  assert batch_size * beam_size == x.shape[0]
  return x.reshape((batch_size, beam_size) + x.shape[1:])


def flat_batch_beam_expand(x, beam_size):
  """Expands the each batch item by beam_size in batch_dimension."""
  return flatten_beam_dim(add_beam_dim(x, beam_size))


def gather_beams(nested, beam_indices, batch_size, new_beam_size):
  """Gathers the beam slices indexed by beam_indices into new beam array.

  Args:
    nested: pytree of arrays or scalars (the latter ignored).
    beam_indices: array of beam_indices
    batch_size: int: size of batch.
    new_beam_size: int: size of _new_ beam dimension.

  Returns:
    New pytree with new beam arrays.
    [batch_size, old_beam_size, ...] --> [batch_size, new_beam_size, ...]
  """
  batch_indices = jnp.reshape(
      jnp.arange(batch_size * new_beam_size) // new_beam_size,
      (batch_size, new_beam_size),
  )

  def gather_fn(x):
    if x.ndim == 0:  # ignore scalars (e.g. cache index)
      return x
    else:
      return x[batch_indices, beam_indices]

  return jax.tree_util.tree_map(gather_fn, nested)


def gather_topk_beams(nested, score_or_log_prob, batch_size, new_beam_size):
  """Gathers the top-k beam slices given by score_or_log_prob array.

  Args:
    nested: pytree of arrays or scalars (the latter ignored).
    score_or_log_prob: [batch_size, old_beam_size] array of values to sort by
      for top-k selection of beam slices.
    batch_size: int: size of batch.
    new_beam_size: int: size of _new_ top-k selected beam dimension

  Returns:
    New pytree with new beam arrays containing top k new_beam_size slices.
    [batch_size, old_beam_size, ...] --> [batch_size, new_beam_size, ...]
  """
  _, topk_indices = lax.top_k(score_or_log_prob, k=new_beam_size)
  topk_indices = jnp.flip(topk_indices, axis=1)
  return gather_beams(nested, topk_indices, batch_size, new_beam_size)


def apply_on_cache(fn, cache, *args, **kwargs):
  """Apply fn(val) only when key is 'keys' or 'val'."""
  new_cache = []
  for layer in cache:
    new_layer = {}
    for key, val in layer.items():
      if key in ['keys', 'values', 'current_index', 'relative_position_bias']:
        val = fn(val, *args, **kwargs)
      new_layer[key] = val
    new_cache.append(new_layer)
  return tuple(new_cache)


# Beam search state:


@flax.struct.dataclass
class BeamState:
  """Holds beam search state data."""

  # The position of the decoding loop in the length dimension.
  cur_index: jax.Array  # scalar int32: current decoded length index
  # The active sequence log probabilities and finished sequence scores.
  live_logprobs: jax.Array  # float32: [batch_size, beam_size]
  finished_scores: jax.Array  # float32: [batch_size, beam_size]
  # The current active-beam-searching and finished sequences.
  live_seqs: jax.Array  # int32: [batch_size, beam_size, max_decode_len]
  finished_seqs: jax.Array  # int32: [batch_size, beam_size,
  #                                         max_decode_len]
  # Records which of the 'finished_seqs' is occupied and not a filler slot.
  finished_flags: jax.Array  # bool: [batch_size, beam_size]
  # The current state of the autoregressive decoding caches.
  cache: Any  # Any pytree of arrays, e.g. flax attention Cache object


def beam_init(seed_token, batch_size, beam_size, max_decode_len, cache):
  """Initializes the beam search state data structure."""
  cur_index0 = jnp.array(0)
  live_logprobs0 = jnp.tile(
      jnp.array([0.0] + [NEG_INF] * (beam_size - 1)), [batch_size, 1]
  )
  finished_scores0 = jnp.ones((batch_size, beam_size)) * NEG_INF

  live_seqs0 = jnp.concatenate(
      [
          jnp.reshape(seed_token, (batch_size, beam_size, 1)),
          jnp.zeros((batch_size, beam_size, max_decode_len - 1), jnp.int32),
      ],
      axis=-1,
  )  # (batch, beam, max_decode_len)

  finished_seqs0 = jnp.zeros((batch_size, beam_size, max_decode_len), jnp.int32)
  finished_flags0 = jnp.zeros((batch_size, beam_size), jnp.bool_)
  beam_cache0 = apply_on_cache(lambda x: jnp.expand_dims(x, axis=0), cache)
  return BeamState(
      cur_index=cur_index0,
      live_logprobs=live_logprobs0,
      finished_scores=finished_scores0,
      live_seqs=live_seqs0,
      finished_seqs=finished_seqs0,
      finished_flags=finished_flags0,
      cache=beam_cache0,
  )


# Beam search routine:


def beam_search_flat(
    seed_token,
    cache,
    tokens_to_logits,
    alpha=BEAM_SEARCH_DEFAULT_ALPHA,
    eos=None,
    max_decode_len=MAX_DECODE_LEN,
    mask=None,
):
  """Beam search for LM.

  inputs and cache is already flat! i.e. first dimention == batch*beam.

  Args:
    seed_token: array: [beam_size, 1] int32 sequence of tokens.
    cache: flax attention cache.
    tokens_to_logits: fast autoregressive decoder function taking single token
      slices and cache and returning next-token logits and updated cache.
    alpha: float: scaling factor for brevity penalty.
    eos: array: [vocab] 1 for end-of-sentence tokens, 0 for not.
    max_decode_len: int: maximum length of decoded translations.
    mask: array: [vocab] binary mask for vocab. 1 to keep the prob, 0 to set the
      prob := 0.

  Returns:
     Tuple of:
       [beam_size, max_decode_len] top-scoring sequences
       [beam_size] beam-search scores.
  """
  # We liberally annotate shape information for clarity below.
  batch_size, beam_size = 1, seed_token.shape[0]
  mask = mask.reshape((1, 1, -1))
  eos = eos.reshape((1, 1, -1))
  mask_bias = (1 - mask) * NEG_INF

  # initialize beam search state
  beam_search_init_state = beam_init(
      seed_token, batch_size, beam_size, max_decode_len, cache
  )

  def beam_search_loop_cond_fn(state):
    """Beam search loop termination condition."""
    # Have we reached max decoding length?
    not_at_end = state.cur_index < max_decode_len - 1

    # Is no further progress in the beam search possible?
    # Get the best possible scores from alive sequences.
    min_brevity_penalty = brevity_penalty(alpha, max_decode_len)
    best_live_scores = state.live_logprobs[:, -1:] / min_brevity_penalty
    # Get the worst scores from finished sequences.
    worst_finished_scores = jnp.min(
        state.finished_scores, axis=1, keepdims=True
    )
    # Mask out scores from slots without any actual finished sequences.
    worst_finished_scores = jnp.where(
        state.finished_flags, worst_finished_scores, NEG_INF
    )
    # If no best possible live score is better than current worst finished
    # scores, the search cannot improve the finished set further.
    search_terminated = jnp.all(worst_finished_scores > best_live_scores)

    # If we're not at the max decode length, and the search hasn't terminated,
    # continue looping.
    return not_at_end & (~search_terminated)

  def beam_search_loop_body_fn(state):
    """Beam search loop state update function."""
    # Collect the current position slice along length to feed the fast
    # autoregressive decoder model.  Flatten the beam dimension into batch
    # dimension for feeding into the model.
    # --> [batch * beam, 1]
    flat_ids = flatten_beam_dim(
        lax.dynamic_slice(
            state.live_seqs, (0, 0, state.cur_index), (batch_size, beam_size, 1)
        )
    )
    # Flatten beam dimension into batch to be compatible with model.
    # {[batch, beam, ...], ...} --> {[batch * beam, ...], ...}
    flat_cache = apply_on_cache(flatten_beam_dim, state.cache)

    # Call fast-decoder model on current tokens to get next-position logits.
    # --> [batch * beam, vocab]
    flat_logits, new_flat_cache = tokens_to_logits(flat_ids, flat_cache)

    # unflatten beam dimension
    # [batch * beam, vocab] --> [batch, beam, vocab]
    logits = unflatten_beam_dim(flat_logits, batch_size, beam_size)

    # Unflatten beam dimension in attention cache arrays
    # {[batch * beam, ...], ...} --> {[batch, beam, ...], ...}
    new_cache = apply_on_cache(
        unflatten_beam_dim, new_flat_cache, batch_size, beam_size
    )

    # Gather log probabilities from logits
    candidate_log_probs = jax.nn.log_softmax(logits)
    # Add new logprobs to existing prefix logprobs.
    # --> [batch, beam, vocab]
    log_probs = candidate_log_probs + jnp.expand_dims(
        state.live_logprobs, axis=2
    )

    # We'll need the vocab size, gather it from the log probability dimension.
    vocab_size = log_probs.shape[2]

    # mask away some tokens.
    log_probs += mask_bias  # [batch,beam,vocab]+[1,1,vocab]

    # Each item in batch has beam_size * vocab_size candidate sequences.
    # For each item, get the top 2*k candidates with the highest log-
    # probabilities. We gather the top 2*K beams here so that even if the best
    # K sequences reach EOS simultaneously, we have another K sequences
    # remaining to continue the live beam search.
    beams_to_keep = 2 * beam_size
    # Flatten beam and vocab dimensions.
    flat_log_probs = log_probs.reshape((batch_size, beam_size * vocab_size))
    # Gather the top 2*K scores from _all_ beams.
    # --> [batch, 2*beams], [batch, 2*beams]
    topk_log_probs, topk_indices = lax.top_k(flat_log_probs, k=beams_to_keep)
    # Recover the beam index by floor division.
    topk_beam_indices = topk_indices // vocab_size
    # Gather 2*k top beams.
    # --> [batch, 2*beams, length]
    topk_seq = gather_beams(
        state.live_seqs, topk_beam_indices, batch_size, beams_to_keep
    )

    # Append the most probable 2*K token IDs to the top 2*K sequences
    # Recover token id by modulo division and expand Id array for broadcasting.
    # --> [batch, 2*beams, 1]
    topk_ids = jnp.expand_dims(topk_indices % vocab_size, axis=2)
    # Update sequences for the 2*K top-k new sequences.
    # --> [batch, 2*beams, length]
    topk_seq = lax.dynamic_update_slice(
        topk_seq, topk_ids, (0, 0, state.cur_index + 1)
    )

    # Update LIVE (in-progress) sequences:
    # Did any of these sequences reach an end marker?
    # --> [batch, 2*beams]
    last_token = topk_seq[:, :, state.cur_index + 1]
    last_token = jax.nn.one_hot(last_token, vocab_size, dtype=jnp.bfloat16)

    # any([batch, 2b, vocab] * [1, 1, vocab], axis=-1) == [batch, 2b]
    newly_finished = jnp.any(last_token * eos, axis=-1)

    # To prevent these newly finished sequences from being added to the LIVE
    # set of active beam search sequences, set their log probs to a very large
    # negative value.
    new_log_probs = topk_log_probs + newly_finished * NEG_INF
    # Determine the top k beam indices (from top 2*k beams) from log probs.
    # --> [batch, beams]
    _, new_topk_indices = lax.top_k(new_log_probs, k=beam_size)
    new_topk_indices = jnp.flip(new_topk_indices, axis=1)
    # Gather the top k beams (from top 2*k beams).
    # --> [batch, beams, length], [batch, beams]
    top_alive_seq, top_alive_log_probs = gather_beams(
        [topk_seq, new_log_probs], new_topk_indices, batch_size, beam_size
    )

    # Determine the top k beam indices from the original set of all beams.
    # --> [batch, beams]
    top_alive_indices = gather_beams(
        topk_beam_indices, new_topk_indices, batch_size, beam_size
    )
    # With these, gather the top k beam-associated caches.
    # --> {[batch, beams, ...], ...}
    top_alive_cache = apply_on_cache(
        gather_beams, new_cache, top_alive_indices, batch_size, beam_size
    )

    # Update FINISHED (reached end of sentence) sequences:
    # Calculate new seq scores from log probabilities.
    new_scores = topk_log_probs / brevity_penalty(alpha, state.cur_index + 1)
    # Mask out the still unfinished sequences by adding large negative value.
    # --> [batch, 2*beams]
    new_scores += (~newly_finished) * NEG_INF

    # Combine sequences, scores, and flags along the beam dimension and compare
    # new finished sequence scores to existing finished scores and select the
    # best from the new set of beams.
    finished_seqs = jnp.concatenate(  # --> [batch, 3*beams, length]
        [state.finished_seqs, topk_seq], axis=1
    )
    finished_scores = jnp.concatenate(  # --> [batch, 3*beams]
        [state.finished_scores, new_scores], axis=1
    )
    finished_flags = jnp.concatenate(  # --> [batch, 3*beams]
        [state.finished_flags, newly_finished], axis=1
    )
    # --> [batch, beams, length], [batch, beams], [batch, beams]
    top_finished_seq, top_finished_scores, top_finished_flags = (
        gather_topk_beams(
            [finished_seqs, finished_scores, finished_flags],
            finished_scores,
            batch_size,
            beam_size,
        )
    )

    return BeamState(
        cur_index=state.cur_index + 1,
        live_logprobs=top_alive_log_probs,
        finished_scores=top_finished_scores,
        live_seqs=top_alive_seq,
        finished_seqs=top_finished_seq,
        finished_flags=top_finished_flags,
        cache=top_alive_cache,
    )

  # Run while loop and get final beam search state.
  final_state = lax.while_loop(
      beam_search_loop_cond_fn, beam_search_loop_body_fn, beam_search_init_state
  )

  # Account for the edge-case where there are no finished sequences for a
  # particular batch item. If so, return live sequences for that batch item.
  # --> [batch]
  none_finished = jnp.any(final_state.finished_flags, axis=1)
  # --> [batch, beams, length]
  finished_seqs = jnp.where(
      none_finished[:, None, None],
      final_state.finished_seqs,
      final_state.live_seqs,
  )
  # --> [batch, beams]
  finished_scores = jnp.where(
      none_finished[:, None],
      final_state.finished_scores,
      final_state.live_logprobs,
  )

  finished_seqs = jnp.reshape(finished_seqs, (beam_size, max_decode_len))
  finished_scores = jnp.reshape(finished_scores, (beam_size,))

  final_cache = apply_on_cache(flatten_beam_dim, final_state.cache)
  return finished_seqs, finished_scores, final_cache

```

##

More general domain language(è®ºæ–‡ 3-4 é¡µ)

AG1:

|          Name           |                                              Meaning                                               |
| :---------------------: | :------------------------------------------------------------------------------------------------: |
|      cong a b c d       |                          ğ´ğµ = ğ¶ğ· (Segment ğ´ğµ is congruent to segment ğ¶ğ·).                          |
|      perp a b c d       |                           ğ´ğµ âŠ¥ ğ¶ğ· (Line ğ´ğµ is perpendicular to line ğ¶ğ·).                           |
|      para a b c d       |                             ğ´ğµ âˆ¥ ğ¶ğ· (Line ğ´ğµ is parallel to line ğ¶ğ·).                              |
|       coll a b c        |             ğ´, ğµ, ğ¶ are collinear (Points ğ´, ğµ, and ğ¶ lie on the same straight line).              |
|     cyclic a b c d      |          ğ´, ğµ, ğ¶, ğ· are concyclic points (Points ğ´, ğµ, ğ¶, and ğ· lie on the same circle).           |
| eqangle a b c d e f g h | âˆ (ğ´ğµ, ğ¶ğ·) = âˆ (ğ¸ğ¹, ğºğ») (Directed angle between ğ´ğµ and ğ¶ğ· is the same as the one between ğ¸ğ¹ and ğºğ»). |
| eqratio a b c d e f g h |                    ğ´ğµ/ğ¶ğ· = ğ¸ğ¹/ğºğ» (The ratio ğ´ğµ/ğ¶ğ· is equal to the ratio ğ¸ğ¹/ğºğ»).                    |
|    aconst a b c d x     |             âˆ (ğ´ğµ, ğ¶ğ·) = ğ‘¥ (Angle between ğ´ğµ and ğ¶ğ· is equal to ğ‘¥, where ğ‘¥ âˆˆ [0, 180)).             |
|    rconst a b c d y     |               ğ´ğµ : ğ¶ğ· = ğ‘¦ (The ratio ğ´ğµ : ğ¶ğ· is equal to ğ‘¦, where ğ‘¦ is a constant).                |

AG2:

1. acompute a b c d means â€œFind the angle between ğ´ğµ and ğ¶ğ·â€.
2. rcompute a b c d means â€œFind the ratio ğ´ğµ/ğ¶ğ·â€.

| Case | Name                        | Subcase             | Syntax for question             |
| :--: | :-------------------------- | :------------------ | :------------------------------ |
|  1   | circle through fixed points | circumcircle a b c  | `? cyclic a b c * : X`          |
|  2   |                             | center a radius b c | `? cong b c a * : X`            |
|  3   | line through fixed points   | line a b            | `? coll a b * : X`              |
|  4   |                             | bline of a b        | `? cong a * b * : X`            |
|  5   |                             | pline of a b c      | `? para b c a * : X`            |
|  6   |                             | tline of a b c      | `? perp b c a * : X`            |
|  7   | point on fixed line         |                     | `? coll a * * : X`              |
|  8   | point on fixed circle       |                     | `? cyclic a * * * : X`          |
|  9   | fixed distance              |                     | `? cong a b * * : X`            |
|  10  | fixed direction             |                     | `? para a b * * : X`            |
|  11  | fixed angle                 |                     | `? eqangle a b a c * * * * : X` |

1. sameclock a b c d e f means the direction ğ´ â†’ ğµ â†’ ğ¶ has the same clockwise direction to
   ğ· â†’ ğ¸ â†’ ğ¹.
2. noverlap a b means ğ´ and ğµ are distinct points.
3. lessthan a b c d means ğ´ğµ < ğ¶ğ·, which is a statement used in the SSA triangle congruence
   theorem.

Notice that, when describing a problem, AG1 uses at most 2 predicates to define a point, i.e. each point is defined as the intersection between at most two objects (line or circle).(AG1 ä½¿ç”¨æœ€å¤šä¸¤ä¸ªè°“è¯æ¥å®šä¹‰ä¸€ä¸ªç‚¹ï¼Œå³æ¯ä¸ªç‚¹æœ€å¤šè¢«å®šä¹‰ä¸ºä¸¤æ¡å¯¹è±¡ï¼ˆçº¿æˆ–åœ†ï¼‰çš„äº¤ç‚¹) This limits AG1 to only constructive problems - problems where all points can be straightforwardly constructed by following their definition order and taking the intersection of two well-defined objects.(AG1 é™åˆ¶åœ¨ä»…èƒ½è§£å†³æ„é€ æ€§é—®é¢˜ä¸Šâ€”â€”å³æ‰€æœ‰ç‚¹å¯ä»¥é€šè¿‡æŒ‰ç…§å®šä¹‰é¡ºåºå¹¶å–ä¸¤æ¡æ˜ç¡®å®šä¹‰çš„å¯¹è±¡çš„äº¤ç‚¹æ¥ç›´æ¥æ„é€ çš„é—®é¢˜) In AG2, we relax this constraint to cover more problems where points can be defined by at least three predicates, making the diagram construction non-trivial.(æˆ‘ä»¬æ”¾å®½äº†è¿™ä¸€é™åˆ¶ï¼Œä»¥è¦†ç›–æ›´å¤šç‚¹å¯ä»¥ç”±è‡³å°‘ä¸‰ä¸ªè°“è¯å®šä¹‰çš„é—®é¢˜, ä»è€Œä½¿å›¾è¡¨æ„é€ å˜å¾—éå¹³å‡¡) Our approach for automating this process is discussed in the next section. All changes described in this section improve the AG domain language coverage from 66 to 88% on all 2000-2024 IMO geometry problems. The remaining 12% contain 3D geometry, inequalities, non-linear equations, and countably many points(å‰©ä½™çš„ 12% åŒ…å«ä¸‰ç»´å‡ ä½•ã€ä¸ç­‰å¼ã€éçº¿æ€§æ–¹ç¨‹å’Œå¯æ•°å¤šä¸ªç‚¹çš„é—®é¢˜) (i.e. problems that have ğ‘› points where ğ‘› is an arbitrary positive integer). All problems (covered and not covered) by AG1 and AG2 can be found on Figure 8. Not covered are referred as "Not attempted".

---

Automated problem formalization and diagram generation. (è‡ªåŠ¨åŒ–é—®é¢˜å½¢å¼åŒ–å’Œå›¾è¡¨ç”Ÿæˆ)

Automated formalization.(è‡ªåŠ¨åŒ–å½¢å¼åŒ–) A major weakness of AlphaGeometry, and other similar neuro-symbolic systems, is the need to manually transform input problems from natural language into a domain specific language.(éœ€è¦æ‰‹åŠ¨å°†è‡ªç„¶è¯­è¨€è¾“å…¥çš„é—®é¢˜è½¬æ¢ä¸ºç‰¹å®šé¢†åŸŸçš„è¯­è¨€) For example, a simple geometry problem in natural language, â€œGiven a triangle ğ´ğµğ¶ with two equal sides ğ´ğµ = ğ´ğ¶, prove that angles ğµ and ğ¶ are equalâ€,(ç»™å®šä¸€ä¸ªä¸‰è§’å½¢ ğ´ğµğ¶ï¼Œå…¶ä¸­ä¸¤è¾¹ç›¸ç­‰ ğ´ğµ = ğ´ğ¶ï¼Œè¯æ˜è§’ ğµ å’Œè§’ ğ¶ ç›¸ç­‰) becomes triangle a b c; a b = a c ? eqangle b a b c c b c a in the AlphaGeometry domain language.

Automating this process, called formalization, is an active area of research (see for example, Jiang et al. (2023); Poiroux et al. (2024); Szegedy (2020); Wu et al. (2022)). It is a significantly more complicated problem compared to a translation between human languages.(è¿™æ¯”åœ¨äººç±»è¯­è¨€ä¹‹é—´è¿›è¡Œç¿»è¯‘è¦å¤æ‚å¾—å¤š) While translation aims to preserve meaning,(è™½ç„¶ç¿»è¯‘æ—¨åœ¨ä¿ç•™æ„ä¹‰) formalization frequently requires re-formulating the original problem into an alternative form,(ä½†å½¢å¼åŒ–é€šå¸¸éœ€è¦å°†åŸå§‹é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºå¦ä¸€ç§å½¢å¼) and sometimes disambiguating the nuances in the original problem statement.(å¹¶ä¸”æœ‰æ—¶è¿˜éœ€è¦æ¾„æ¸…åŸå§‹é—®é¢˜é™ˆè¿°ä¸­çš„ç»†å¾®å·®åˆ«) Automated formalization (auto-formalization), therefore, demands significant background knowledge and problem-solving skills on its own.(è‡ªåŠ¨åŒ–å½¢å¼åŒ–ï¼ˆauto-formalizationï¼‰æœ¬èº«å°±éœ€è¦å¤§é‡çš„èƒŒæ™¯çŸ¥è¯†å’Œé—®é¢˜è§£å†³æŠ€å·§) Given that recently foundation models started demonstrating such capabilities, we use one such model, Gemini Team Gemini (2024), to automate the problem formalization for AlphaGeometry. We start by manually translating several dozens of geometry problems into the AG language. Then we use these examples to write a few-shot prompt asking Gemini to translate a given geometry problem from natural language into the AG language. We query Gemini five times with this prompt followed by another Gemini call asking to combine these results into one final answer.(è¦æ±‚ Gemini å°†ç»™å®šçš„å‡ ä½•é—®é¢˜ä»è‡ªç„¶è¯­è¨€ç¿»è¯‘æˆ AG è¯­è¨€ã€‚æˆ‘ä»¬ç”¨è¿™ä¸ªæç¤ºå‘ Gemini æŸ¥è¯¢äº”æ¬¡ï¼Œéšåå†è¿›è¡Œä¸€æ¬¡ Gemini è°ƒç”¨ä»¥å°†è¿™äº›ç»“æœåˆå¹¶ä¸ºä¸€ä¸ªæœ€ç»ˆç­”æ¡ˆ) With this approach we are able to formalize 30 out of 39 formalizable IMO 2000-2024 geometry problems. For easier geometry problems, it is very consistent and makes almost no mistakes.

ä¹Ÿå°±æ˜¯è¿˜æ˜¯è¦ç”¨ Gemini

Automated diagram generation. Another manual part of our pipeline was diagram generation. In AG1, each point is defined by at most two basic predicates recalled in Table 1, the problem is therefore defined constructively and diagrams can be generated automatically.(æ¯ä¸ªç‚¹æœ€å¤šç”±è¡¨ 1 ä¸­åˆ—å‡ºçš„ä¸¤ä¸ªåŸºæœ¬è°“è¯å®šä¹‰ï¼Œå› æ­¤é—®é¢˜å¯ä»¥æ„é€ æ€§åœ°å®šä¹‰ï¼Œå¹¶ä¸”å¯ä»¥è‡ªåŠ¨ç”Ÿæˆå›¾è¡¨) In AG2, we allow one or multiple points being defined simultaneously by an arbitrary number of predicates, allowing us to also cover non-constructive problems.(æˆ‘ä»¬å…è®¸ä¸€ä¸ªæˆ–å¤šä¸ªç‚¹åŒæ—¶è¢«ä»»æ„æ•°é‡çš„è°“è¯å®šä¹‰ï¼Œä»è€Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ¶µç›–éæ„é€ æ€§é—®é¢˜) Consider a non-constructive problem statement, â€œLet ğ´ğµğ¶ be a triangle with incenter ğ¼, such that ğ¼ğ´ = 2ğ¼ğµ ...â€,(è®¾ ğ´ğµğ¶ æ˜¯ä¸€ä¸ªå†…å¿ƒä¸º ğ¼ çš„ä¸‰è§’å½¢ï¼Œä½¿å¾— ğ¼ğ´ = 2ğ¼ğµ ...) here point ğ¼ is not only defined as an incenter, i.e. the intersection of two internal bisectors, but also defined by a third predicate ğ¼ğ´ = 2ğ¼ğµ and there is no general strategy to construct such four points.(è¿™é‡Œç‚¹ ğ¼ ä¸ä»…è¢«å®šä¹‰ä¸ºå†…å¿ƒï¼Œå³ä¸¤ä¸ªå†…è§’å¹³åˆ†çº¿çš„äº¤ç‚¹ï¼Œè¿˜è¢«ç¬¬ä¸‰ä¸ªè°“è¯ ğ¼ğ´ = 2ğ¼ğµ å®šä¹‰ï¼Œè€Œæ²¡æœ‰æ„é€ è¿™å››ä¸ªç‚¹çš„ä¸€èˆ¬ç­–ç•¥) Since AG2 covers non-constructive problems, diagram construction becomes a non-trivial part of the pipeline and generally requires human intervention.(ç”±äº AG2 æ¶µç›–éæ„é€ æ€§é—®é¢˜ï¼Œå›¾è¡¨ç”Ÿæˆæˆä¸ºæµç¨‹ä¸­çš„ä¸€ä¸ªéå¹³å‡¡éƒ¨åˆ†ï¼Œå¹¶ä¸”é€šå¸¸éœ€è¦äººå·¥å¹²é¢„) Similar to Krueger et al. (2021), we propose the following algorithm to automatically generate diagrams given non-constructive problem specifications:

Let Â¯ğ‘¥ âˆˆ â„2ğ‘› be a vector representing all coordinates of all points. We encode every constraint ğ‘ in the diagram, including the goal, as ğ‘“ğ‘ (Â¯ğ‘¥) = 0 with a nonlinear function ğ‘“ğ‘. We numerically search for a suitable Â¯ğ‘¥ in two steps. First, we run the ADAM optimization on the mean-squared error loss Î£ğ‘âˆˆğ¶ ğ‘“ğ‘ (Â¯ğ‘¥) where ğ¶ is the set of all the constraints, together with a non-degeneracy loss. For every two points ğ´, ğµ, we add the loss of the form 1/(|ğ´ğµ|2 + ğœ–), and an ğ¿2 normalization on all points to prevent their value from becoming too large. After the loss in the ADAM optimization meets a certain threshold,(ä¸€æ—¦ ADAM ä¼˜åŒ–ä¸­çš„æŸå¤±è¾¾åˆ°æŸä¸ªé˜ˆå€¼) we stop caring about the non-degeneracy,(æˆ‘ä»¬å°±ä¸å†å…³æ³¨éé€€åŒ–æŸå¤±) and switch from a gradient descent optimization to the Gauss-Newton-Levenberg method(å¹¶ä»æ¢¯åº¦ä¸‹é™ä¼˜åŒ–åˆ‡æ¢åˆ° Gauss-Newton-Levenberg method) to look for a numerical solution of a combined under- and over-determined system of nonlinear equations.(ä»¥å¯»æ‰¾ä¸€ä¸ªç»„åˆçš„æ¬ å®šå’Œè¿‡å®šéçº¿æ€§æ–¹ç¨‹ç»„çš„æ•°å€¼è§£)

This two-stage optimization method builds upon the methodology introduced in Krueger et al. (2021). While the first stage remains unchanged, we incorporate a novel second stage.(ä½†æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°é¢–çš„ç¬¬äºŒé˜¶æ®µ) This addition addresses the practical limitations encountered when tuning the gradient descent optimization in the original method, where achieving a consistently satisfactory error margin proved challenging.(è¿™ä¸€æ”¹è¿›è§£å†³äº†åœ¨åŸå§‹æ–¹æ³•ä¸­è°ƒä¼˜æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æ—¶é‡åˆ°çš„å®é™…é™åˆ¶ï¼Œå³éš¾ä»¥å§‹ç»ˆè¾¾åˆ°æ»¡æ„çš„è¯¯å·®èŒƒå›´) We benchmark this method on 44 IMO problems formalized in AG language (see Figure 8) and are able to find diagrams for 41 of them. We run the two-stage convergence procedure in multiple parallel processes, and in a loop which restarts and generates another random initial configuration after a failure. This way, 40 / 44 problems got their diagram generated within 1 hour using approximately 40 processes for each problem (many problems got their diagram within seconds, on the first try). For the remaining 4 problems, we run the same procedure longer and with more parallelization. This way, we also obtained a diagram for IMO-2011-6 after 400 minutes using 3333 processes.

---

è®ºæ–‡ 6-8 é¡µ

Continuous Paragraph:
A symbolic engine, DDAR (Deductive Database Arithmetic Reasoning)(æ¼”ç»æ•°æ®åº“ç®—æœ¯æ¨ç†), is crucial for AlphaGeometry. It computes the deduction closureâ€”all deducible facts from initial factsâ€”by iteratively applying deduction rules.(é€šè¿‡è¿­ä»£åº”ç”¨æ¼”ç»è§„åˆ™æ¥è®¡ç®—æ¼”ç»é—­åŒ…â€”â€”ä»åˆå§‹äº‹å®æ¨å¯¼å‡ºçš„æ‰€æœ‰å¯æ¨å¯¼äº‹å®) DDAR drives both training data generation and test-time proof search, where speed is essential.(DDAR é©±åŠ¨ç€è®­ç»ƒæ•°æ®ç”Ÿæˆå’Œæµ‹è¯•æ—¶çš„è¯æ˜æœç´¢ï¼Œé€Ÿåº¦è‡³å…³é‡è¦) Faster data generation enables larger datasets and more aggressive filtering, while faster proof search allows more extensive exploration, increasing the chance of finding a solution.(æ›´å¿«çš„æ•°æ®ç”Ÿæˆå¯ä»¥å®ç°æ›´å¤§çš„æ•°æ®é›†å’Œæ›´ç§¯æçš„è¿‡æ»¤ï¼Œè€Œæ›´å¿«çš„è¯æ˜æœç´¢å¯ä»¥è¿›è¡Œæ›´å¹¿æ³›çš„æ¢ç´¢ï¼Œä»è€Œå¢åŠ åœ¨ç»™å®šæ—¶é—´å†…æ‰¾åˆ°è§£å†³æ–¹æ¡ˆçš„æœºä¼š) Three key DDAR improvements are discussed: handling double points, a faster algorithm, and a faster implementation.(ä¸‰ä¸ªä¸»è¦æ”¹è¿›ï¼šå¤„ç†é‡åˆç‚¹ã€æ›´å¿«çš„ç®—æ³•å’Œæ›´å¿«çš„å®ç°)

DDAR1 lacked the ability to handle double points (two points with the same coordinates but different names), a crucial feature for complex problems. For example, proving that the intersection of two lines a and b (point X) lies on a circle Ï‰ might involve proving that the intersection of a and Ï‰ (point X') lies on b, then proving X = X', and thus X lies on Ï‰. This requires constructing the auxiliary point X' and proving its equivalence to X.(DDAR1 ç¼ºä¹å¤„ç†é‡åˆç‚¹ï¼ˆä¸¤ä¸ªåæ ‡ç›¸åŒä½†åç§°ä¸åŒçš„ç‚¹ï¼‰çš„èƒ½åŠ›ï¼Œè¿™æ˜¯è§£å†³å¤æ‚é—®é¢˜çš„å…³é”®ç‰¹å¾ã€‚ä¾‹å¦‚ï¼Œè¯æ˜ä¸¤æ¡ç›´çº¿ a å’Œ b çš„äº¤ç‚¹(X)ä½äºåœ† Ï‰ ä¸Šï¼Œå¯èƒ½éœ€è¦è¯æ˜ a å’Œ Ï‰ çš„äº¤ç‚¹(X')ä½äº b ä¸Šï¼Œç„¶åè¯æ˜ X = X'ï¼Œä»è€Œè¯æ˜ X ä½äº Ï‰ ä¸Šã€‚è¿™éœ€è¦æ„é€ è¾…åŠ©ç‚¹ X'å¹¶è¯æ˜å…¶ä¸ X ç­‰ä»·)

The DDAR1 algorithm applied each rule to all point combinations, involving a polynomial-time candidate search and an exponential-time clause matching. The search for similar triangles, for instance, could theoretically be O(N^8). DDAR2 hard-codes essential rules, reducing queries to the AR sub-engine to at most cubic complexity. It also discards explicit angle and distance rules, as these are handled automatically by the AR engine. DDAR2 efficiently finds similar triangles by hashing point "shapes" and cyclic quadrilaterals by hashing (A, B, âˆ AXB) values.(DDAR1 ç®—æ³•å°†æ¯ä¸ªè§„åˆ™åº”ç”¨äºæ‰€æœ‰ç‚¹çš„ç»„åˆï¼ŒåŒ…æ‹¬å¤šé¡¹å¼æ—¶é—´çš„å€™é€‰æœç´¢å’ŒæŒ‡æ•°æ—¶é—´çš„å­å¥åŒ¹é…ã€‚ä¾‹å¦‚ï¼Œæœç´¢ç›¸ä¼¼ä¸‰è§’å½¢çš„ç†è®ºå¤æ‚åº¦å¯èƒ½ä¸º O(N^8)ã€‚DDAR2 å¯¹åŸºæœ¬è§„åˆ™è¿›è¡Œç¡¬ç¼–ç ï¼Œå°†å¯¹ AR å­å¼•æ“çš„æŸ¥è¯¢å‡å°‘åˆ°æœ€å¤šä¸‰æ¬¡å¤æ‚åº¦ã€‚å®ƒè¿˜ä¸¢å¼ƒäº†æ˜¾å¼çš„è§’åº¦å’Œè·ç¦»è§„åˆ™ï¼Œå› ä¸ºè¿™äº›è§„åˆ™ç”± AR å¼•æ“è‡ªåŠ¨å¤„ç†ã€‚DDAR2 é€šè¿‡å“ˆå¸Œç‚¹â€œå½¢çŠ¶â€æœ‰æ•ˆåœ°æ‰¾åˆ°ç›¸ä¼¼ä¸‰è§’å½¢ï¼Œå¹¶é€šè¿‡å“ˆå¸Œ(A, B, âˆ AXB)å€¼æ‰¾åˆ°åœ†å†…æ¥å››è¾¹å½¢)

Finally, the core computation (Gaussian Elimination) was implemented in C++ using pybind11, resulting in a 300x speedup compared to DDAR1. Benchmarking on 25 unsolved IMO problems, DDAR2 averaged 3.45 seconds per problem, compared to DDAR1's 1179.57 seconds.(æœ€åï¼Œä½¿ç”¨ pybind11 åœ¨ C++ä¸­å®ç°äº†æ ¸å¿ƒè®¡ç®—ï¼ˆé«˜æ–¯æ¶ˆå…ƒï¼‰ï¼Œä¸ DDAR1 ç›¸æ¯”ï¼Œé€Ÿåº¦æé«˜äº† 300 å€ã€‚åœ¨ 25 ä¸ªæœªè§£å†³çš„ IMO é—®é¢˜ä¸Šè¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼ŒDDAR2 æ¯ä¸ªé—®é¢˜çš„å¹³å‡è€—æ—¶ä¸º 3.45 ç§’ï¼Œè€Œ DDAR1 çš„å¹³å‡è€—æ—¶ä¸º 1179.57 ç§’)

---

è®ºæ–‡ 8-10 é¡µ

Better synthetic training data

Supplementing the symbolic engine with a language model was a key to AG1 success, bringing the solve rate from 14 (pure deduction proofs) to 25 out of 30 selected IMO problems. This language model was trained on a large amount of algorithmically generated synthetic data.(è¯¥è¯­è¨€æ¨¡å‹æ˜¯æ ¹æ®å¤§é‡ç®—æ³•ç”Ÿæˆçš„åˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒçš„) In AG2 we use the same procedure.

ä¹Ÿå°±æ˜¯æ•°æ®ä¸è¶³çš„æƒ…å†µä¸‹, AGI åˆæˆæ•°æ®ä¿è¯ä»åˆå§‹äº‹å®æ¨å¯¼å‡ºçš„æ‰€æœ‰å¯æ¨å¯¼äº‹å®

Similar to AG1, our synthetic data generation method starts by sampling a random diagram, and uses the symbolic engine to deduce all possible facts from it. For each of the deduced facts,(ç„¶åä½¿ç”¨ç¬¦å·å¼•æ“ä»ä¸­æ¨æ–­å‡ºæ‰€æœ‰å¯èƒ½çš„äº‹å®) a traceback algorithm is used to extract the corresponding premises, auxiliary points, and deduction steps that prove the fact.(ä½¿ç”¨å›æº¯ç®—æ³•æå–ç›¸åº”çš„å‰æã€è¾…åŠ©ç‚¹å’Œè¯æ˜è¯¥äº‹å®çš„æ¨å¯¼æ­¥éª¤) Our data generation method deliberately avoids the use of human-crafted problems as initial diagram seeds,(æˆ‘ä»¬çš„æ•°æ®ç”Ÿæˆæ–¹æ³•åˆ»æ„é¿å…ä½¿ç”¨äººä¸ºé—®é¢˜ä½œä¸ºåˆå§‹å›¾ç§å­) and strictly starts from random diagrams. This design choice eliminates the risk of data contamination(è¿™ç§è®¾è®¡é€‰æ‹©æ¶ˆé™¤äº†æ•°æ®æ±¡æŸ“çš„é£é™©) and allows for the exploration of theorem distributions that may extend beyond established human knowledge.(å¹¶å…è®¸æ¢ç´¢å¯èƒ½è¶…å‡ºäººç±»ç°æœ‰çŸ¥è¯†èŒƒå›´çš„å®šç†åˆ†å¸ƒ) This approach contrasts with methods like TongGeometry, which rely on human expertise and existing problem diagrams to guide and filter data generation.(åè€…ä¾èµ–äººç±»ä¸“ä¸šçŸ¥è¯†å’Œç°æœ‰é—®é¢˜å›¾æ¥æŒ‡å¯¼å’Œè¿‡æ»¤æ•°æ®ç”Ÿæˆ) In AG2, we keep using random diagrams as initial seeds and continue to push for better synthetic training data.

**Larger, more complex diagrams and better data distribution**.(æ›´å¤§ã€æ›´å¤æ‚çš„å›¾è¡¨å’Œæ›´å¥½çš„æ•°æ®åˆ†å¸ƒ) First of all, we scale up resources for data generation, and do more careful re-balancing of the data distribution. As demonstrated on Figure 2, compared to AG1, AG2:

â€¢ Explores random diagrams at twice the size, allowing for extracting much more complex
problems.(æ¢ç´¢ä¸¤å€å¤§å°çš„éšæœºå›¾è¡¨ï¼Œå…è®¸æå–æ›´å¤æ‚çš„å†…å®¹é—®é¢˜)

â€¢ Produces theorems at up to 2x more complex, i.e. number of points and premises.(äº§ç”Ÿå¤æ‚ç¨‹åº¦é«˜è¾¾ 2 å€çš„å®šç†ï¼Œå³ç‚¹å’Œå‰æçš„æ•°é‡)

â€¢ Produces up to 10x more complex proofs, i.e. 10x more proof steps.(ç”Ÿæˆå¤šè¾¾ 10 å€å¤æ‚çš„è¯æ˜ï¼Œå³å¤š 10 å€çš„è¯æ˜æ­¥éª¤)

â€¢ Has a more balanced data distribution between question types.(é—®é¢˜ç±»å‹ä¹‹é—´çš„æ•°æ®åˆ†å¸ƒæ›´åŠ å¹³è¡¡)

â€¢ Has a more balanced data distribution between problems with and without auxiliary points.(åœ¨æœ‰è¾…åŠ©ç‚¹å’Œæ²¡æœ‰è¾…åŠ©ç‚¹çš„é—®é¢˜ä¹‹é—´æœ‰æ›´å¹³è¡¡çš„æ•°æ®åˆ†å¸ƒ)

More types of theorems. Besides generating theorems that prove classic statements such as â€œAB =
CDâ€, AG2 data generating algorithm also produces problems of â€œlocusâ€ type, i.e. asserting statements such as â€œWhen X moves on line/circle Y, then Z moves on a fixed line/circle Tâ€.(å½“ X åœ¨ç›´çº¿/åœ† Y ä¸Šç§»åŠ¨æ—¶ï¼Œåˆ™ Z åœ¨å›ºå®šçº¿/åœ† T ä¸Šç§»åŠ¨) Introduced in Section 2, these statements are not supported in the AG1 data generation algorithm, as there is no notion of movement and movement dependency.(AG1 æ•°æ®ç”Ÿæˆç®—æ³•ä¸æ”¯æŒè¿™äº›è¯­å¥ï¼Œå› ä¸ºæ²¡æœ‰è¿åŠ¨å’Œè¿åŠ¨ä¾èµ–æ€§) In AG2, we record the movement dependency for each point ğ‘‹ during random diagram generation through a function ğ‘ƒ(.) with the following definition:

ğ‘ƒ(ğ´): the set of points that control the movements of ğ´, where ğ´ is a point or a set of points, defined in a constructive problem statement. Two examples of ğ‘ƒ are presented in Table 3 and all
cases where locus-type statements are detected are shown in Table 5.

|        If        |               Then                |
| :--------------: | :-------------------------------: |
| a = midpoint b c | d = midpoint a c<br>ğ‘ƒ(ğ‘‘) = {ğ‘, ğ‘} |
| a = on_line b c  |         ğ‘ƒ(ğ‘) = {ğ‘, ğ‘, ğ‘}          |

...

**Faster data generation algorithm**.(æ›´å¿«çš„æ•°æ®ç”Ÿæˆç®—æ³•) We also improved the speed of the data generation algorithm.(æˆ‘ä»¬è¿˜æé«˜äº†æ•°æ®ç”Ÿæˆç®—æ³•çš„é€Ÿåº¦) Recall that in AG1, we first run deduction closure on random diagrams,(æˆ‘ä»¬é¦–å…ˆåœ¨éšæœºå›¾ä¸Šè¿è¡Œæ¨æ¼”é—­åŒ…) and â€œtraceback" to obtain the minimal problem and minimal proof that proves each fact in the closure.(ç„¶åâ€œå›æº¯â€ä»¥è·å¾—
è¯æ˜ç»“è®ºä¸­æ¯ä¸ªäº‹å®çš„æœ€å°é—®é¢˜å’Œæœ€å°è¯æ®) To obtain the minimal problem in AG1, we have to exhaustively remove different subsets of points from the problem and rerun DDAR to check provability(ä¸ºäº†è·å¾—æœ€å°çš„ AG1 ä¸­çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¿…é¡»ä»é—®é¢˜ä¸­å½»åº•åˆ é™¤ç‚¹çš„ä¸åŒå­é›†ï¼Œå¹¶ä¸”é‡æ–°è¿è¡Œ DDAR ä»¥æ£€æŸ¥å¯è¯æ˜æ€§). Such a search can find the subset with the smallest cardinality,(è¿™æ ·çš„æœç´¢å¯ä»¥æ‰¾åˆ°åŸºæ•°æœ€å°çš„å­é›†) however as an exponential search(æŒ‡æ•°æœç´¢), it is infeasible for a larger number of points.(å¯¹äºå¤§é‡çš„ç‚¹æ¥è¯´æ˜¯ä¸å¯è¡Œçš„) Therefore, we switched to a greedily discarding algorithm(è´ªå©ªä¸¢å¼ƒç®—æ³•) shown in Figure 3, which uses just a linear number of checks for whether a set of points suffices to prove the goal.(ä»…ä½¿ç”¨çº¿æ€§æ•°æ£€æŸ¥ä¸€ç»„ç‚¹æ˜¯å¦è¶³ä»¥è¯æ˜ç›®æ ‡) The greedy algorithm is guaranteed to find a minimal set of points with respect to inclusion as long as the check is monotonic (if ğ´ âŠ† ğµ, then check_provable(ğ´) â‡’ check_provable(ğµ)).(åªè¦æ£€æŸ¥æ˜¯å•è°ƒçš„ï¼ˆå¦‚æœ ğ´ âŠ† ğµï¼Œåˆ™ check_provable(ğ´) â‡’ check_provable(ğµ)ï¼‰ï¼Œè´ªå¿ƒç®—æ³•å°±èƒ½ä¿è¯æ‰¾åˆ°å…³äºåŒ…å«çš„æœ€å°ç‚¹é›†) In reality, we also require the pruned set to remain closed under construction dependencies (so that we can still run a random construction).(è¿˜è¦æ±‚å‰ªæé›†åœ¨æ„é€ ä¾èµ–é¡¹ä¸‹ä¿æŒå°é—­ï¼ˆä»¥ä¾¿æˆ‘ä»¬ä»ç„¶å¯ä»¥è¿è¡Œéšæœºæ„é€ ï¼‰) If we incorporate this condition into the check_provable predicate,(æˆ‘ä»¬å°†æ­¤æ¡ä»¶åˆå¹¶åˆ° check_provable è°“è¯ä¸­) it stops being monotonic.(å®ƒå°±ä¸å†æ˜¯å•è°ƒçš„) This difficulty can be fixed by processing the points via the algorithm from Figure 3 in a reversetopological order (first points that do not depend on any other points, and last the initial points of the construction).(ä»¥é€†æ‹“æ‰‘é¡ºåºå¤„ç†ç‚¹æ¥è§£å†³ï¼ˆç¬¬ä¸€ä¸ªç‚¹ä¸ä¾èµ–äºä»»ä½•å…¶ä»–ç‚¹ï¼Œæœ€åä¸€ä¸ªç‚¹æ˜¯æ„é€ çš„åˆå§‹ç‚¹ï¼‰)

```py
def prune_point s (
    point s : set [ Point ] ,
    check_provable : Ca l l a b l e [ [ set [ Point ] ] , bool ] ) :
    pruned = set ( point s )
    for p in r e v e r s e _ t o p o l o g i c a l ( point s ) :
    i f check_provable ( pruned âˆ’ {p } ) :
    pruned = pruned âˆ’ {p}
    return
```

è¿™æ˜¯ä¼ªä»£ç å§, æš‚æ—¶æ²¡æ‰¾åˆ°æºç é‡Œç›¸ä¼¼çš„...

---

**Novel search algorithm**(æ–°é¢–çš„æœç´¢ç®—æ³•)

In AG1, we use a simple beam search to discover proofs.(æˆ‘ä»¬ä½¿ç”¨ç®€å•çš„é›†æŸæœç´¢æ¥å‘ç°è¯æ®) In AG2, we design a novel search algorithm, in which several differently configured beam searches are executed in parallel and are allowed to help each other through a knowledge sharing mechanism (see Figure 4).(æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§æ–°é¢–çš„æœç´¢ç®—æ³•ï¼Œå…¶ä¸­å¹¶è¡Œæ‰§è¡Œå¤šä¸ªä¸åŒé…ç½®çš„æ³¢æŸæœç´¢ï¼Œå¹¶å…è®¸é€šè¿‡çŸ¥è¯†å…±äº«æœºåˆ¶ç›¸äº’å¸®åŠ©) To improve the robustness of our system we use multiple different language models for each search tree configuration.(æˆ‘ä»¬ä¸ºæ¯ä¸ªæœç´¢æ ‘é…ç½®ä½¿ç”¨å¤šç§ä¸åŒçš„è¯­è¨€æ¨¡å‹) We call this search algorithm Shared Knowledge Ensemble of Search Trees (SKEST).(æˆ‘ä»¬å°†æ­¤æœç´¢ç®—æ³•ç§°ä¸ºæœç´¢æ ‘å…±äº«çŸ¥è¯†é›†æˆ (SKEST))

It works as follows. In each search tree, a node corresponds to one attempt at auxiliary construction followed by one attempt of the symbolic engine run.(åœ¨æ¯ä¸ªæœç´¢æ ‘ä¸­ï¼Œä¸€ä¸ªèŠ‚ç‚¹å¯¹åº”äºè¾…åŠ©æ„é€ çš„ä¸€æ¬¡å°è¯•ï¼Œéšåæ˜¯ç¬¦å·å¼•æ“è¿è¡Œçš„ä¸€æ¬¡å°è¯•) If the attempt succeeds, all search trees terminate.(å¦‚æœå°è¯•æˆåŠŸï¼Œæ‰€æœ‰æœç´¢æ ‘éƒ½ä¼šç»ˆæ­¢) If the attempt fails, the node will write down the facts that the symbolic engine managed to prove into a shared facts database.(å¦‚æœå°è¯•å¤±è´¥ï¼ŒèŠ‚ç‚¹ä¼šå°†ç¬¦å·å¼•æ“è®¾æ³•è¯æ˜çš„äº‹å®å†™å…¥å…±äº«äº‹å®æ•°æ®åº“) These shared facts are filtered such that they are not the auxiliary point specific to the node itself, but only relevant to the original problem.(è¿™äº›å…±äº«äº‹å®è¢«è¿‡æ»¤ï¼Œä½¿å¾—å®ƒä»¬ä¸æ˜¯ç‰¹å®šäºèŠ‚ç‚¹æœ¬èº«çš„è¾…åŠ©ç‚¹ï¼Œè€Œä»…ä¸åŸå§‹é—®é¢˜ç›¸å…³) This way, these facts can also be useful to the other nodes in the same search tree, and the nodes in different search trees.(è¿™æ ·ï¼Œè¿™äº›äº‹å®å¯¹äºåŒä¸€æœç´¢æ ‘ä¸­çš„å…¶ä»–èŠ‚ç‚¹ä»¥åŠä¸åŒæœç´¢æ ‘ä¸­çš„èŠ‚ç‚¹ä¹Ÿå¾ˆæœ‰ç”¨) Below we list various types of search trees, which we employ to make sure different parts of the search space are explored effectively:(ç¡®ä¿æœ‰æ•ˆåœ°æ¢ç´¢æœç´¢ç©ºé—´çš„ä¸åŒéƒ¨åˆ†)

â€¢ "Classic" search tree: the same beam tree search used in AG1, where a language model is asked to produce one auxiliary point at each node.(â€œç»å…¸â€æœç´¢æ ‘ï¼šä¸ AG1 ä¸­ä½¿ç”¨çš„ç›¸åŒçš„æŸæ ‘æœç´¢ï¼Œå…¶ä¸­è¦æ±‚è¯­è¨€æ¨¡å‹åœ¨æ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆä¸€ä¸ªè¾…åŠ©ç‚¹)

â€¢ Tree predicting multiple auxiliary points at each node: a language model is allowed to produce as many auxiliary points as it wants at each tree node. Recall that this is possible because our LM is trained to produce full proofs, starting with auxiliary points and followed by the deduction steps2. Note that even though we want our models to generate all necessary auxiliary points in one query, in practice we observe the need to call the model multiple times given previously produced auxiliary points. Allowing the model to produce multiple auxiliary points accelerates finding a solution and effectively increases the tree search depth(åœ¨æ¯ä¸ªèŠ‚ç‚¹å¤„é¢„æµ‹å¤šä¸ªè¾…åŠ©ç‚¹çš„æ ‘ï¼šå…è®¸è¯­è¨€æ¨¡å‹åœ¨æ¯ä¸ªæ ‘èŠ‚ç‚¹å¤„äº§ç”Ÿå°½å¯èƒ½å¤šçš„è¾…åŠ©ç‚¹ã€‚å›æƒ³ä¸€ä¸‹ï¼Œè¿™æ˜¯å¯èƒ½çš„ï¼Œå› ä¸ºæˆ‘ä»¬çš„ LM ç»è¿‡è®­ç»ƒå¯ä»¥ç”Ÿæˆå®Œæ•´çš„è¯æ˜ï¼Œä»è¾…åŠ©ç‚¹å¼€å§‹ï¼Œç„¶åæ˜¯æ¼”ç»æ­¥éª¤ 2ã€‚è¯·æ³¨æ„ï¼Œå³ä½¿æˆ‘ä»¬å¸Œæœ›æ¨¡å‹åœ¨ä¸€ä¸ªæŸ¥è¯¢ä¸­ç”Ÿæˆæ‰€æœ‰å¿…è¦çš„è¾…åŠ©ç‚¹ï¼Œä½†å®é™…ä¸Šæˆ‘ä»¬è§‚å¯Ÿåˆ°éœ€è¦åœ¨ç»™å®šå…ˆå‰ç”Ÿæˆçš„è¾…åŠ©ç‚¹çš„æƒ…å†µä¸‹å¤šæ¬¡è°ƒç”¨æ¨¡å‹ã€‚å…è®¸æ¨¡å‹äº§ç”Ÿå¤šä¸ªè¾…åŠ©ç‚¹åŠ é€Ÿæ±‚è§£å¹¶æœ‰æ•ˆå¢åŠ æ ‘æœç´¢æ·±åº¦)

â€¢ Tree predicting different types of aux points uniformly. Recall that LM outputs for auxiliary points look like x00 a : cong a b c d (00) coll a e f (01), i.e. â€œconstruct point a such that a b = c d and a e f are collinear". Typically to predict aux points we prompt the language model with the first token x00 and let it generate the rest. Here, instead, we prompt the LM with x00 a : cong, x00 a : coll, x00 a : cyclic, x00 a : perp, etc. to force uniform distribution across the first 4 tokens, and then let the LM generate the rest.(ç»Ÿä¸€é¢„æµ‹ä¸åŒç±»å‹è¾…åŠ©ç‚¹çš„æ ‘ã€‚å›æƒ³ä¸€ä¸‹ï¼Œè¾…åŠ©ç‚¹çš„ LM è¾“å‡ºçœ‹èµ·æ¥åƒ x00 a : cong a b c d (00) coll a e f (01)ï¼Œå³â€œæ„é€ ç‚¹ aï¼Œä½¿å¾— a b = c d å’Œ a e f å…±çº¿â€ã€‚é€šå¸¸ä¸ºäº†é¢„æµ‹è¾…åŠ©ç‚¹ï¼Œæˆ‘ä»¬ç”¨ç¬¬ä¸€ä¸ªæ ‡è®° x00 æç¤ºè¯­è¨€æ¨¡å‹ï¼Œå¹¶è®©å®ƒç”Ÿæˆå…¶ä½™çš„æ ‡è®°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç”¨ x00 a : cong, x00 a : coll æç¤º LMï¼Œ x00 a ï¼šå¾ªç¯ï¼Œx00 a ï¼šperp ç­‰å¼ºåˆ¶åœ¨å‰ 4 ä¸ªä»¤ç‰Œä¸Šå‡åŒ€åˆ†å¸ƒï¼Œç„¶åè®© LM ç”Ÿæˆå…¶ä½™ä»¤ç‰Œ)

â€¢ Deep but narrow tree (e.g. beam size 64 and depth 10).(æ·±è€Œçª„çš„æ ‘ï¼ˆä¾‹å¦‚æ¢å°ºå¯¸ 64 å’Œæ·±åº¦ 10ï¼‰)

â€¢ Shallow but wide tree (e.g. beam size 512 and depth 4).(æµ…ä½†å®½çš„æ ‘ï¼ˆä¾‹å¦‚æ¢å°ºå¯¸ 512 å’Œæ·±åº¦ 4ï¼‰)

è¿™ä¸ªå°±è¦ç»“åˆæºç çœ‹äº† [beam_search.py](#beam_search-py)

---

å·²ç»æ™•äº†, è¿˜æœ‰ä¸€èŠ‚ Better language model ä¸çœ‹äº†
