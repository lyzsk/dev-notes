# Simple test-time scaling

@see: https://arxiv.org/abs/2501.19393

äº®ç‚¹: test-time compute å°±æ˜¯å¤šèŠ±æ—¶é—´åœ¨ resoning çš„è¿‡ç¨‹, ç»“åˆäº†ä½é¢„ç®—

åªç”¨äº† SFT æ²¡ç”¨ RL(Reinforcement learning), deepseek r1 ç”¨çš„æ˜¯ RL è¿›è¡Œæ¨ç†

æ¨ç†æ•°æ®å°‘, æ¨ç†æ•°æ®æ ‡æ³¨éš¾, æ¯”è¾ƒé€‚åˆ RL, åšè¶‹åŠ¿å­¦ä¹ , è€Œéæ˜ç¡®çš„ç›®æ ‡

æ‰€ä»¥åªç”¨ SFT å°±æ˜¯è¯´æ˜è¦ç²¾å¿ƒæŒ‘é€‰æ•°æ®(åªç”¨äº† 1000 samles)(We show that training on only 1,000 samples with next-token prediction and controlling thinking duration via a simple test-time technique we refer to as budget forcing leads to a strong reasoning model that scales in performance with more test-time compute.)

å¦‚ä½•ä» 50K æ•°æ®é‡Œé€‰æ‹© 1000 æ¡, ç­›é€‰æ•°æ®ç»éªŒ

æé™å‹ä½é¢„ç®—, è®­ç»ƒæˆæœ¬ 26min 16H100 1000 samples(ç”¨ Google Gemini Flash Thinking API ç”Ÿæˆ), e.g. æ¨ç†è¿‡ç¨‹ç”¨ Gemini ç”Ÿæˆ reasoning trace and response, å¸®å¿™åˆæˆæ•°æ®, ç”¨ Claude 3.5 Sonnet è¿›è¡Œæ•°æ®åˆ†ç±». å°±æ˜¯ç”¨å…¶ä»–æ¨¡å‹çš„èƒ½åŠ›å¸®å¿™å¤„ç†æ•°æ®, æ¯”å¦‚ deepseek è’¸é¦, è¿™ç§äº’ç›¸äº¤äº’æ•°æ®, ä½†æ˜¯è’¸é¦å°±é¢ä¸´ teacher æ¨¡å‹éœ€è¦ logic, åƒ GPT è¿™ç§é—­æºæ— æºä»£ç çš„å°±æ²¡åŠæ³•ç”¨äº†

1. Recently, OpenAIâ€™s o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. å°±æ˜¯æ¨ç†è¿‡ç¨‹ä¸çŸ¥é“æ€ä¹ˆå®ç°
2. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. æ¢ç´¢æ¨ç†è¿‡ç¨‹ä¸­çš„å¢å¼ºæ¥æå‡è®­ç»ƒèƒ½åŠ›, å°±æ˜¯ 59K->1k samples, criteria å°±æ˜¯ difficulty, diversity, quality
3. Second, we develop budget forcing to control test-time compute by forcefully terminating the modelâ€™s thinking process or lengthening it by appending â€œWaitâ€ multiple times to the modelâ€™s generation when it tries to end. å°±æ˜¯ç”¨ `budget forcing` è¿™ä¸ªæ–¹æ³•æ¥æ§åˆ¶æ˜¯å¦ç»§ç»­æ¨ç†è¿˜æ˜¯ç»™å‡º final answer, å…¶å®å°±æ˜¯ max token, æ•°é‡åˆ°æå€¼å°±åœæ­¢ thinking

ä»¥å‰æ˜¯ multi agent approach, å¤šä¸ªæ¨¡å‹å»æ•£å¸ƒæ•°æ®, ç°åœ¨å‘ç°ä¸éœ€è¦äº†, åªè¦ SFT äº†

æ€æ ·è®©ä¸€ä¸ªæ¨¡å‹å»æ€è€ƒ, è®©å®ƒå¼€å§‹çš„æ—¶å€™ä¸å›ç­”é—®é¢˜, ç›´æ¥ç”Ÿæˆä¸€ä¸ª context, å…ˆ thinking, ç„¶ååˆ° max token å† answer, ä¹Ÿå°±æ˜¯å…ˆæ˜¯ç”Ÿæˆ(context+thinkg) \* n, æœ€å select, å°±å®Œæˆäº†æ¨ç†

We collect an initial 59,029 questions from 16 diverse sources following three guiding principles: **Quality**, where datasets should be of high qualityâ€”we always inspect samples and ignore datasets with issues such as poor formatting; **Difficulty**, where datasets should be challenging and require significant reasoning effort; and **Diversity**, where datasets should stem from different fields to cover a wide range of reasoning tasks. We collect datasets of two categories:

-   Curation of existing datasets(ä¸»è¦æ˜¯ online websites æ•°å­¦é—®é¢˜), ç„¶ååœ¨åŸºç¡€ä¸Šä¸ºäº†æå‡ diversity, æ·»åŠ äº† Questions spanning Astronomy, Biology, Chemistry, Computer Science, Geography, Mathematics, and Physics are collected from various Olympiads, ensuring a diverse and challenging set of problems that require advanced reasoning and problem-solving skills. These questions are designed to test deep understanding and application of knowledge across multiple disciplines, making them ideal for generating exercises that push the boundaries of critical thinking and analytical abilities. ç„¶åè¿˜åŠ äº† covering English, Law, and Logic çš„ AGIEval.

-   New datasets in quantitative reasoning, **s1-prob** consists of 182 questions from the probability section of Stanford Universityâ€™s Statistics Departmentâ€™s PhD Qualifying Exams, accompanied by handwritten solutions that cover difficult proofs(æ¶µç›–å›°éš¾çš„è¯æ˜è¿˜è¦æ˜¯æ‰‹å†™çš„, ä¹Ÿå°±æ˜¯ä¸å¤ªå¥½æ ¼å¼åŒ–çš„). The probability qualifying exam is held yearly and requires professional-level mathematical problem-solving, making it an excellent resource for advanced learners. Complementing this, **s1-teasers** comprises 23 challenging brain-teasers(è„‘ç»æ€¥è½¬å¼¯) commonly used in interview questions for quantitative trading positions(é‡åŒ–äº¤æ˜“å¤´å¯¸), which test logical reasoning and creative problem-solving skills. Together, these datasets provide a comprehensive range of problems that span rigorous academic challenges and real-world applications, ensuring a well-rounded preparation for both theoretical and practical problem-solving scenarios. We only take examples with the highest difficulty level("Hard"). è¯´ç™½äº†è·Ÿç«èµ›æ•°æ®å·®ä¸å¤š?

å‚è€ƒä¸Šæ–‡è¯´çš„:(I) If the model generates more thinking tokens than a desired limit, we forcefully end the thinking process by appending an end-of-thinking token delimiter. This approach ensures that the model transitions to generating its answer, preventing excessive or unnecessary reasoning.(II) Conversely, if we want the model to spend more test-time compute on a problem, we suppress the generation of the end-of-thinking token delimiter and instead append â€œWaitâ€ to the modelâ€™s current reasoning trace. This encourages further exploration and deeper reasoning, allowing the model to potentially arrive at a more refined or accurate solution. These two strategies provide flexible control over the modelâ€™s reasoning process, balancing efficiency and thoroughness based on the specific requirements of the task.

ä¹Ÿå°±æ˜¯å¾ˆå¤š token æ˜¯éœ€è¦ delimiter çš„

è¿˜æœ‰å°±æ˜¯ä¸€äº›æ¯”è¾ƒåŸºç¡€çš„é—®é¢˜, è°ƒç”¨çš„æ¨¡å‹å¯ä»¥å›ç­”çš„å‡ºæ¥å°±å¯ä»¥ç­›é™¤äº†

Claude ä¸»è¦æ˜¯ diversity æ¯ä¸ªé¢†åŸŸåˆ’åˆ† 1 æ¡æ•°æ®

ç„¶åæœ€é‡è¦çš„å°±æ˜¯è¿™äº›æ•°æ®æ•´åˆæ˜¯é æ¥ä¸‹æ¥åæ–‡è¯´çš„:

For each question, we generate a reasoning trace and solution using the Google Gemini Flash Thinking API(Google, 2024), extracting its reasoning trace and response. This process yields 59,000 triplets, each consisting of a question, a generated reasoning trace, and a generated solution. Examples from our dataset are detailed in Â§C.2. To ensure the integrity and quality of the data, we decontaminate all samples against our evaluation questionsâ€”MATH500, GPQA Diamond, and AIME24â€”using 8-grams, and we further deduplicate the data to remove any redundant or overlapping entries. This rigorous preprocessing step ensures that the dataset is clean, unique, and well-suited for training and evaluation purposes.

ä¸­é—´ä¸€æ­¥æ­¥ç­›é€‰æ•°æ®

**Quality** We first remove any questions where we ran into any API errors reducing our dataset to 54,116 samples. Next, we filter out low-quality examples by checking if they contain any string patterns with formatting issues, such as ASCII art diagrams, non-existent image references, or inconsistent question numbering reducing our dataset to 51,581 examples. From this pool, we identify 384 samples for our final 1,000 samples from datasets that we perceive as high-quality and not in need of further filtering(see Â§B.4 for details).

è¿™é‡Œå¾ˆä¸»è§‚, å¯èƒ½å› ä¸ºç”¨åˆ°äº† online æ•°æ®, è¿‡æœŸä¸å­˜åœ¨ç­‰é—®é¢˜, è¿˜æœ‰æ ¼å¼åŒ–çš„æ ‡å‡†å‹é—®é¢˜?

**Difficulty** For difficulty, we use two indicators: model performance and reasoning trace length(æ¨¡å‹æ€§èƒ½å’Œæ¨ç†è½¨è¿¹é•¿åº¦). We evaluate two models on each question: Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct(Qwen et al., 2024), with correctness assessed by Claude 3.5 Sonnet comparing each attempt against the reference solution(see Â§B.3 for the grading protocol). We measure the token length of each reasoning trace to indicate problem difficulty using the Qwen2.5 tokenizer(Qwen2.5 åˆ†è¯å™¨æµ‹é‡æ¯ä¸ªæ¨ç†è½¨è¿¹çš„æ ‡è®°é•¿åº¦ ä»¥è¡¨ç¤ºé—®é¢˜çš„éš¾åº¦). This relies on the assumption that more difficult problems require more thinking tokens. Based on the grading, we remove questions that either Qwen2.5-7B-Instruct or Qwen2.5-32B-Instruct can solve correctly and thus may be too easy(ä¹Ÿå°±æ˜¯ç§»å‡º Qwen èƒ½æ­£ç¡®è§£å†³çš„é—®é¢˜, å› ä¸ºå¯èƒ½è¿‡äºç®€å•). By using two models we reduce the likelihood of an easy sample slipping through our filtering due to a rare mistake on an easy question of one of the models. This brings our total samples down to 24,496(ä¹Ÿå°±æ˜¯ç”¨äº† 2 ä¸ªç°æœ‰æ¨¡å‹ä¹Ÿæ‰å‡åŠ), setting the stage for the next round of subsampling based on diversity. While filtering with these two models may be optimized for our setup as we will also use Qwen2.5-32B-Instruct as our model to finetune, the idea of model-based filtering generalizes to other setups.

**Diversity** To quantify diversity we classify each question into specific domains using Claude 3.5 Sonnet based on the Mathematics Subject Classification(MSC) system(e.g., geometry, dynamic systems, real analysis, etc.)(å‡ ä½•ã€åŠ¨æ€ç³»ç»Ÿã€å®åˆ†æç­‰) from the American Mathematical Society.1 The taxonomy focuses on topics in mathematics but also includes other sciences such as biology, physics, and economics. To select our final examples from the pool of 24,496 questions, we first choose one domain uniformly at random.(é¦–å…ˆå‡åŒ€éšæœºåœ°é€‰æ‹©ä¸€ä¸ªé¢†åŸŸ) Then, we sample one problem from this domain according to a distribution that favors longer reasoning traces(see Â§B.4 for details) as motivated in Difficulty. We repeat this process until we have 1,000 total samples.(æ ¹æ®ä¸€ä¸ªå€¾å‘äºæ›´é•¿æ¨ç†è½¨è¿¹çš„åˆ†å¸ƒ, ä»è¯¥é¢†åŸŸä¸­é‡‡æ ·ä¸€ä¸ªé—®é¢˜ï¼Œå¦‚éš¾åº¦éƒ¨åˆ†æ‰€è¿°, ä¹Ÿå°±æ˜¯è¿™é‡Œåˆç”¨äº†ä¸€æ¬¡ä¸Šé¢ 2 ä¸ªæ¨¡å‹, ç„¶ååå¤åˆ©ç”¨ç¼©å‡è‡³ 1000)

benchmark çœ‹ æ¯” o1-preview å¼º, æ¯” o1 å·®, å¤Ÿäº†, å› ä¸ºèŠ‚çœäº†èµ„æº, è¿˜æ˜¯ 32B çš„

å·¥ç¨‹åŒ–éƒ¨åˆ†:

We take a model that has already been pretrained and instruction tuned and further finetune it for reasoning.(é€‰å–ä¸€ä¸ªå·²ç»é¢„è®­ç»ƒå’ŒæŒ‡ä»¤è°ƒä¼˜çš„æ¨¡å‹, å¹¶è¿›ä¸€æ­¥å¯¹å…¶è¿›è¡Œæ¨ç†è°ƒä¼˜) Specifically, we use Qwen2.5-32B-Instruct(Qwen et al., 2024), which on math tasks generally matches or outperforms the larger Qwen2.5-72B-Instruct(Qwen et al., 2024) or other open models(Dubey et al., 2024; Groeneveld et al., 2024; Muennighoff et al., 2024). We use token delimiters to separate the thinking stage from the answering stage.(ä½¿ç”¨æ ‡è®°åˆ†éš”ç¬¦æ¥åŒºåˆ†æ€è€ƒé˜¶æ®µå’Œå›ç­”é˜¶æ®µ) We enclose the thinking stage with <| im_start |>think and <| im_start |>answer; both preceded and followed by a newline. Samples from our dataset are in Â§C.2. We use basic fine-tuning hyperparameters: we train for 5 epochs with a batch size of 16 for a total of 315 gradient steps.(5 epochs, 16 batch size, 315 gradient steps å¯ä»¥è¯´æ˜¯éå¸¸éå¸¸ç‰›é€¼äº†) We train in bfloat16 precision with a learning rate of 1ğ‘’ âˆ’ 5 warmed up linearly for 5%(16 steps) and then decayed to 0 over the rest of training(299 steps) following a cosine schedule. We use the AdamW optimizer(Loshchilov & Hutter, 2019) with ğ›½1 = 0.9, ğ›½2 = 0.95 and weight decay of 1ğ‘’ âˆ’ 4.(å­¦ä¹ ç‡å¼€å§‹å€¼ 1e-5 ç”¨ Adam optimizer å‰ 16step 5% çº¿æ€§é¢„çƒ­, å 299step æ ¹æ®ä½™å¼¦è°ƒåº¦è¡°å‡è‡³ 0, å­¦ä¹ ç‡å±…ç„¶è¿˜å¯ä»¥è¡°å‡åˆ° 0?!) We do not compute loss on questions, only on reasoning traces and solutions. We ensure the sequence length is large enough to avoid cutting off any samples; a setting we ablate in Â§C.1. The training takes just 26 minutes on 16 NVIDIA H100 GPUs.

è¡¥å……: ä¸¤ä¸ªå‘é‡ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯è´Ÿæ•°, æ‰€ä»¥æ‰èƒ½è¡°å‡

@see: https://juejin.cn/post/7434419051331649573

@see: https://cloud.tencent.com/developer/information/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%A4%E4%B8%AA%E5%90%91%E9%87%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E6%80%A7%E6%98%AF%E8%B4%9F%E7%9A%84%EF%BC%9F

æ ‡è®°ä»£ç :

```py
"""Compute avg thinking steps"""

import sys
import json

path = sys.argv[1] # e.g. qwen_20241129-012159-32steps_32kctxt/ckpts__qwen_20241129-012159/samples_openai_math_2024-12-01T01-56-47.156277.jsonl
try:
    # allowed_steps = int(path.split('steps')[0].split('-')[-1].split('_')[-1])
    allowed_steps = int(path.split('step')[-1].split('/')[0].split('forcing')[0])
    allowed_tokens = None
except:
    try:
        allowed_tokens = int(path.split('tokens')[0].split('-')[-1].split('_')[-1])
        allowed_steps = None
    except:
        allowed_steps, allowed_tokens = None, None
        print("No steps/tokens in path; Assuming it was run without steps/tokens limit")

tokens = True
total_steps = 0
step_lens = []
total_lens = []
total_lens_with_sep = []
total_lens_answer = []
samples = 0
samples_too_many_steps = 0
samples_too_many_tokens = 0
samples_too_many_thinking_tokens = 0
samples_without_answer = 0
#samples_without_answer_but_steps = 0
samples_without_thinking = 0
correct = []

if tokens:
    from transformers import AutoTokenizer
    tok = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-32B-Instruct")

with open(path) as f: # JSONL file
    for line in f:
        data = json.loads(line)

        if isinstance(data['filtered_resps'][0], list):
            data['filtered_resps'][0] = data['filtered_resps'][0][0]

        ### <|im_start|>step format ###
        # # total_steps += data['filtered_resps'][0].count('<|reserved_special_token_2|>')
        # total_steps += data['filtered_resps'][0].count('<|im_start|>step')
        # samples_without_answer += int('<|im_start|>answer' not in data['filtered_resps'][0])
        # #samples_without_answer_but_steps += int(('<|im_start|>answer' not in data['filtered_resps'][0]) and('<|im_start|>step' in data['filtered_resps'][0]))
        # samples_without_thinking += int('<|im_start|>step' not in data['filtered_resps'][0])
        # samples_too_many_steps += int(data['filtered_resps'][0].count('<|im_start|>step') > allowed_steps)
        # step_lens.extend([len("\n".join(step.split("\n")[1:])) for step in data['filtered_resps'][0].split('<|im_start|>step')[1:-1]])
        # samples += 1

        if tokens and("qwq" in path):
            total_lens_with_sep.append(len(tok.tokenize(data['filtered_resps'][0])))

            too_long = int(len(tok.tokenize(data['arguments']['gen_args_0']['arg_0'] + data['filtered_resps'][0])) > 32760)
            samples_too_many_tokens += too_long

            no_answer = int(('Answer:' not in data['filtered_resps'][0]) and('\\boxed' not in data['filtered_resps'][0]))
            samples_without_answer += no_answer
        elif tokens and allowed_steps:
            total_steps += data['filtered_resps'][0].count(' steps left\n')
            samples_without_thinking += int(' steps left\n' not in data['filtered_resps'][0])
            samples_too_many_steps += int(data['filtered_resps'][0].count(' steps left\n') > allowed_steps)
            # if int(data['filtered_resps'][0].count(' steps left\n') > allowed_steps):
            #     import pdb; pdb.set_trace()
            step_lens_tmp = [len(tok.tokenize("\n".join(step.split("\n")[1:]))) for step in data['filtered_resps'][0].split('<|im_start|>')[1:-1]]
            total_lens.extend([sum(step_lens_tmp)])
            total_lens_with_sep.extend([len(tok.tokenize(data['filtered_resps'][0]))])
            if "<|im_start|>answer" in data['filtered_resps'][0]:
                total_lens_answer.append(len(tok.tokenize(data['filtered_resps'][0].split('<|im_start|>answer')[1])))
            else:
                total_lens_answer.append(0)
            step_lens.extend(step_lens_tmp)

            #import pdb; pdb.set_trace()
            too_long = int(len(tok.tokenize(data['arguments']['gen_args_0']['arg_0'] + data['filtered_resps'][0])) > 32760) # 32768
            samples_too_many_tokens += too_long

            no_answer = int(('Answer:' not in data['filtered_resps'][0]) and('\\boxed' not in data['filtered_resps'][0]))
            samples_without_answer += no_answer

            # Worth checking
            #if no_answer and not(too_long):
            #    import pdb; pdb.set_trace()
        elif tokens:
            steps = data['filtered_resps'][0].split('<|im_start|>answer')[0].split("\n")[1:-1]
            thinking = "\n".join(steps)
            if '<|im_start|>answer' in data['filtered_resps'][0]:
                answer = data['filtered_resps'][0].split('<|im_start|>answer')[-1]
                if "\n" in answer:
                    answer = "\n".join(answer.split("\n")[1:])
            else:
                answer = ""

            total_steps += data['filtered_resps'][0].split('<|im_start|>answer')[0].count('\n') - 1
            samples_without_thinking += int('<|im_start|>think' not in data['filtered_resps'][0])

            step_lens_tmp = [len(tok.tokenize(step)) for step in steps]
            step_lens.extend(step_lens_tmp)

            thinking_tokens = len(tok.tokenize(thinking))
            total_lens.append(thinking_tokens)

            #if sum(step_lens_tmp) == 30119:
            #   import pdb; pdb.set_trace()

            if allowed_tokens:
                samples_too_many_thinking_tokens += int(thinking_tokens > allowed_tokens)

            total_lens_with_sep.append(len(tok.tokenize(data['filtered_resps'][0])))
            total_lens_answer.append(len(tok.tokenize(answer)))

            too_long = int(len(tok.tokenize(data['arguments']['gen_args_0']['arg_0'] + data['filtered_resps'][0])) > 32760)
            if too_long:
                import pdb; pdb.set_trace()
            samples_too_many_tokens += too_long

            no_answer = int(('Answer:' not in data['filtered_resps'][0]) and('\\boxed' not in data['filtered_resps'][0]))
            samples_without_answer += no_answer

            # Worth checking
            #if not(no_answer) and too_long:
            #    import pdb; pdb.set_trace()
            #if no_answer and not(too_long):
            #    import pdb; pdb.set_trace()
            # if no_answer:
            #    import pdb; pdb.set_trace()

        ### <|im_start|>13 steps left format ###
        else:
            if allowed_steps:
                total_steps += data['filtered_resps'][0].count(' steps left\n')
                samples_without_thinking += int(' steps left\n' not in data['filtered_resps'][0])
                samples_too_many_steps += int(data['filtered_resps'][0].count(' steps left\n') > allowed_steps)
                step_lens_tmp = [len("\n".join(step.split("\n")[1:])) for step in data['filtered_resps'][0].split('<|im_start|>')[1:-1]]
                total_lens.extend([sum(step_lens_tmp)])
                step_lens.extend(step_lens_tmp)
            else:
                # Approximation
                total_steps += data['filtered_resps'][0].split('<|im_start|>answering\n')[0].count('\n') - 1
                samples_without_thinking += int('<|im_start|>thinking\n' not in data['filtered_resps'][0])
                step_lens.extend([len("\n".join(step.split("\n")[1:])) for step in data['filtered_resps'][0].split('<|im_start|>answering\n')[0].split('\n')[1:-1]])
                total_lens.extend([len(data['filtered_resps'][0].split('<|im_start|>answering\n')[0])])

            samples_without_answer += int('<|im_start|>answer' not in data['filtered_resps'][0])
            #samples_without_answer_but_steps += int(('<|im_start|>answer' not in data['filtered_resps'][0]) and('<|im_start|>step' in data['filtered_resps'][0]))

        samples += 1
        correct.append(data['exact_match'])

print("# samples:", samples)
if tokens:
    print("avg steps:", round(total_steps / samples, 1))
    if step_lens:
        print("avg step tokens:", round(sum(step_lens) / len(step_lens), 1))
    if total_lens:
        print("min total thinking tokens:", min(total_lens))
        print("max total thinking tokens:", max(total_lens))
        print("sorted total thinking tokens:", sorted(total_lens))
        correct_sorted = [correct[i] for i in sorted(range(len(total_lens)), key=lambda k: total_lens[k])]
        print("sorted correct:", correct_sorted)
        print("avg total thinking tokens:", round(sum(total_lens) / len(total_lens), 1))
    if total_lens_answer:
        print("avg total answer tokens:", round(sum(total_lens_answer) / len(total_lens_answer), 1))
    print("avg total tokens:", round(sum(total_lens_with_sep) / len(total_lens_with_sep), 1))
    if allowed_steps:
        print("samples w/ too many steps:", samples_too_many_steps)
    else:
        print("samples w/ too many thinking tokens:", samples_too_many_thinking_tokens)
    print("samples w/ too many tokens:", samples_too_many_tokens)
    print("samples w/o answer:", samples_without_answer)
    print("samples w/o thinking:", samples_without_thinking)
elif allowed_steps:
    print("avg steps:", round(total_steps / samples, 1))
    print("avg step len:", round(sum(step_lens) / len(step_lens), 1))
    print("avg total thinking len:", round(sum(total_lens) / len(total_lens), 1))
    print("samples w/ too many steps:", samples_too_many_steps)
    print("samples w/o 'Answer:':", samples_without_answer)
    #print("samples w/o answer but steps:", samples_without_answer_but_steps)
    print("samples w/o thinking:", samples_without_thinking)
else:
    print("avg steps(approx via \\n):", round(total_steps / samples, 1))
    print("avg step len(approx via \\n):", round(sum(step_lens) / len(step_lens), 1))
    print("avg total thinking len:", round(sum(total_lens) / len(total_lens), 1))
    print("samples w/o answer:", samples_without_answer)
    print("samples w/o thinking:", samples_without_thinking)

```

è®­ç»ƒ epoch, step, lr, etc. ä»£ç :

```py
# Copyright(c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
import os
import sys
import tempfile
import time
from pathlib import Path
import itertools

import tqdm

import torch
import torch.nn.functional as F
from torch.utils.data.distributed import DistributedSampler

import torch._inductor.config
import torch._dynamo.config

torch._inductor.config.coordinate_descent_tuning = True
torch._inductor.config.triton.unique_kernel_names = True
torch._inductor.config.fx_graph_cache = True  # Experimental feature to reduce compilation times, will be on by default in future

torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

try:
    import wandb
except ImportError:
    wandb = None

# support running without installing as a package
wd = Path(__file__).parent.parent.resolve()
sys.path.append(str(wd))

from models.model import Transformer, set_global_compile_mode
from models.tp import(
    maybe_init_dist,
    initialize_model_parallel,
    get_model_parallel_group,
    get_model_parallel_world_size,
    get_data_parallel_world_size,
    clip_grad_norm_,
    compute_vocab_parallel_logprobs,
)

from data_utils.common_utils import manual_seed
from data_utils.data_utils_sft import make_sft_data_module
from data_utils.tokenizer_utils import FakePreTrainedTokenizer

from training_utils.hf_argparser import HfArgumentParser
from training_utils.training_args import TrainingArguments
from training_utils.checkpoint_hook import(
    checkpoint_hook,
    get_latest_checkpoint_path,
    load_checkpoint,
    load_model_from_from_ckpt,
)
from training_utils.trainer_utils import(
    create_optimizer,
    create_fsdp_model_for_finetune,
    get_cosine_schedule_with_warmup,
)

IGNORE_INDEX = -100


def model_forward(model, x, input_pos):
    return model(x, input_pos, fully_causal=True)


def model_forward_with_loss(
    model: Transformer,
    input_ids: torch.Tensor,
    labels: torch.Tensor,
) -> torch.Tensor:
    """
    Compute the loss for a given model and prompts.
    """
    # create an empty tensor of the expected final shape and fill in the current tokens
    batch_size, T = input_ids.size(0), input_ids.size(1)

    device = input_ids.device
    with torch.device(device):
        model.setup_caches(max_batch_size=batch_size, max_seq_length=T, kv_cache=False)
    # create an empty tensor of the expected final shape and fill in the current tokens
    input_pos = torch.arange(0, T, device=device)

    with torch.backends.cuda.sdp_kernel(
        enable_flash=True, enable_math=False, enable_mem_efficient=False
    ):
        logits = model_forward(model, input_ids, input_pos)

    with torch.autocast(device_type="cuda", enabled=False):
        logits = logits.float()
        logits = logits[..., :-1, :].contiguous()
        labels = labels[..., 1:].contiguous()

        if model.vocab_parallel:
            loss = -compute_vocab_parallel_logprobs(
                logits.view(-1, logits.size(-1)),
                labels.view(-1),
                ignore_index=IGNORE_INDEX,
                reduction="mean",
            )
        else:
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                labels.view(-1),
                reduction="mean",
            )
    return loss


def encode_tokens(tokenizer, string, bos=True, device="cuda"):
    tokens = tokenizer.encode(string)
    if bos:
        tokens = [tokenizer.bos_id()] + tokens
    return torch.tensor(tokens, dtype=torch.int, device=device)


def main(
    args: TrainingArguments,
) -> None:
    """Finetune a model on a given dataset."""
    checkpoint_path = args.checkpoint_path
    sft_checkpoint_path = args.sft_checkpoint_path
    compile = args.compile
    assert checkpoint_path.is_file(), checkpoint_path

    if sft_checkpoint_path is not None:
        assert sft_checkpoint_path.is_dir(), sft_checkpoint_path

    tokenizer_path = checkpoint_path.parent / "tokenizer.model"
    if not tokenizer_path.is_file():
        tokenizer_path = checkpoint_path.parent

    set_global_compile_mode(compile)

    global print
    device_id = maybe_init_dist()
    use_tp = device_id is not None
    if use_tp:
        group_size = args.tensor_parallel_size or torch.distributed.get_world_size()
        initialize_model_parallel(group_size)
        torch.distributed.barrier()
        intra_node_group = get_model_parallel_group()

        if device_id != 0:
            # only print on rank 0
            print = lambda *args, **kwargs: None

    if args.report_to == "wandb" and wandb is not None:
        if device_id == 0:
            wandb_logging_dir = os.path.join(
                tempfile.gettempdir(), f"{os.getuid()}_wandb"
            )
            if not os.path.exists(wandb_logging_dir):
                os.makedirs(wandb_logging_dir, exist_ok=True)
            os.environ["WANDB_DIR"] = wandb_logging_dir
            wandb.init(
                name=args.wandb_name,
                project=args.wandb_project,
                entity=args.wandb_entity,
                resume="allow",
                magic=True,
                dir=wandb_logging_dir,
                force=True,
            )
            wandb.config.update(vars(args))

    device = "cuda"
    precision = args.param_dtype

    print("Loading model ...")
    t0 = time.time()
    model = load_model_from_from_ckpt(
        checkpoint_path,
        sft_checkpoint_path,
        device,
        precision,
        use_tp,
        requires_grad=True,
        sequence_parallel=args.sequence_parallel,
        vocab_parallel=args.vocab_parallel,
    )

    torch.cuda.synchronize()
    print(f"Time to load model: {time.time() - t0:.02f} seconds")

    tokenizer = FakePreTrainedTokenizer(tokenizer_path)

    data_module = make_sft_data_module(
        tokenizer=tokenizer,
        args=args,
    )
    train_dataset = data_module["train_dataset"]
    data_collator = data_module["data_collator"]

    model_size = sum(
        [
            p.numel() * p.dtype.itemsize
            for p in itertools.chain(model.parameters(), model.buffers())
        ]
    )

    print(f"Model size: {model_size / 1e6:.02f} MB")
    manual_seed(args.seed)

    sampler = None
    if use_tp:
        sampler = DistributedSampler(
            train_dataset,
            shuffle=True,
            drop_last=True,
        )

    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=args.per_device_train_batch_size,
        shuffle=(sampler is None),
        sampler=sampler,
        num_workers=0,
        pin_memory=True,
        collate_fn=data_collator,
    )

    if args.print_training_examples:
        print("Training examples:")
        cnt = 16
        for batch in train_loader:
            print(
                "Input:",
                tokenizer.decode(
                    batch["input_ids"][0].tolist(), skip_special_tokens=False
                ),
            )
            print(
                "Target:",
                tokenizer.decode(
                    batch["labels"][0].tolist(), skip_special_tokens=False
                ),
            )
            cnt -= 1
            if cnt == 0:
                break

    if compile:
        model = torch.compile(model)

    trainable_param_names = [
        name for name, param in model.named_parameters() if param.requires_grad
    ]

    use_fsdp = False

    if get_data_parallel_world_size() > 1:
        use_fsdp = True
        model = create_fsdp_model_for_finetune(args, model)
        print("Using FSDP ...")
        print(model)

    optimizer = create_optimizer(
        args,
        model=model,
        optimizer_cpu_offload=args.optimizer_cpu_offload,
        model_cpu_offload=False,
    )

    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        warmup_epochs=len(train_loader) * args.warmup_ratio,
        max_epochs=len(train_loader) * args.num_train_epochs,
        warmup_start_ratio=0.0,
        eta_min_ratio=args.lr_eta_min / args.learning_rate,
    )

    resume_from_checkpoint = None
    resume_epoch = 0
    resume_global_step = 0

    if args.resume_from_checkpoint:
      (
            resume_from_checkpoint,
            resume_epoch,
            resume_global_step,
        ) = get_latest_checkpoint_path(args.save_dir)

        if resume_from_checkpoint is not None:
            print(
                f"Resuming from checkpoint: {resume_from_checkpoint}(epoch {resume_epoch}, global step {resume_global_step})"
            )
            load_checkpoint(
                resume_from_checkpoint, model, optimizer, scheduler, use_fsdp=use_fsdp
            )

    micro_train_batch_size =(
        args.micro_train_batch_size or args.per_device_train_batch_size
    )

    assert(
        args.per_device_train_batch_size % micro_train_batch_size == 0
    ), f"per_device_train_batch_size({args.per_device_train_batch_size}) must be divisible by micro_train_batch_size({micro_train_batch_size})"
    accumulate_steps = args.per_device_train_batch_size // micro_train_batch_size

    print(
        "Batch size per GPU for training: {}\n".format(
            args.per_device_train_batch_size
        ),
        "Micro batch size for training: {}\n".format(micro_train_batch_size),
        "Gradient accumulation steps: {}\n".format(accumulate_steps),
    )

    micro_train_batch_size = micro_train_batch_size * torch.distributed.get_world_size()

    epoch_length = len(train_loader)

    if args.do_train:
        print("Starting training ...")
        t0 = time.time()
        for epoch in tqdm.trange(
            args.num_train_epochs, desc="Epoch", disable=device_id != 0
        ):
            if sampler is not None:
                train_loader.sampler.set_epoch(epoch)
            pbar = tqdm.tqdm(
                enumerate(train_loader),
                desc="Iteration",
                disable=device_id != 0,
                total=len(train_loader),
            )
            for it, batch in pbar:
                global_step = epoch * epoch_length + it
                if global_step < resume_global_step:
                    continue

                torch.cuda.synchronize()
                model.zero_grad()

                input_ids = batch["input_ids"].to(device=device)
                labels = batch["labels"].to(device=device)

                input_ids, labels = prepare_batch(
                    input_ids,
                    labels,
                    tokenizer=tokenizer,
                    use_tp=use_tp,
                    sync_group=intra_node_group,
                )

                loss_scale = 1.0 / accumulate_steps
                for ex_idx in range(0, input_ids.size(0), micro_train_batch_size):
                    if ex_idx + micro_train_batch_size < input_ids.size(0):
                        with torch.cuda.amp.autocast(dtype=args.compute_dtype):
                            loss = model_forward_with_loss(
                                model,
                                input_ids[ex_idx : ex_idx + micro_train_batch_size],
                                labels[ex_idx : ex_idx + micro_train_batch_size],
                            )
                      (loss_scale * loss).backward()
                    else:
                        with torch.cuda.amp.autocast(dtype=args.compute_dtype):
                            loss = model_forward_with_loss(
                                model,
                                input_ids[ex_idx:],
                                labels[ex_idx:],
                            )
                      (loss_scale * loss).backward()
                        grad_norm = clip_grad_norm_(model, 1.0)
                        optimizer.step()
                        scheduler.step()

                if it % 5 == 0:
                    loss_copy = loss.detach().clone()
                    torch.distributed.all_reduce(loss_copy)
                    avg_loss =(loss_copy / torch.distributed.get_world_size()).item()
                    grad_norm_copy = grad_norm.detach().clone().item()

                    if device_id == 0:
                        if args.report_to == "wandb" and wandb is not None:
                            wandb.log(
                                {
                                    "loss": avg_loss,
                                    "learning_rate": scheduler.get_last_lr()[0],
                                    "epoch": epoch,
                                    "step": it,
                                    "grad_norm": grad_norm_copy,
                                },
                                step=global_step,
                            )
                        else:
                            # Just print to stdout.
                            print(
                                {
                                    "loss": avg_loss,
                                    "learning_rate": scheduler.get_last_lr()[0],
                                    "epoch": epoch,
                                    "step": it,
                                    "grad_norm": grad_norm_copy,
                                }
                            )

                checkpoint_hook(
                    args,
                    model,
                    optimizer,
                    scheduler,
                    epoch,
                    global_step,
                    epoch_length,
                    use_fsdp=use_fsdp,
                    trainable_param_names=trainable_param_names,
                )

        torch.cuda.synchronize()

        epoch = args.num_train_epochs

        checkpoint_hook(
            args,
            model,
            optimizer,
            scheduler,
            epoch,
            epoch * epoch_length,
            epoch_length,
            use_fsdp=use_fsdp,
            trainable_param_names=trainable_param_names,
        )

        print(f"Time to train: {time.time() - t0:.02f} seconds")


def prepare_batch(input_ids, labels, tokenizer, use_tp, sync_group):
    pad_id = tokenizer.pad_id
    unk_id = tokenizer.unk_id
    # if pad_id < 0, replace pad_id with unk_id
    labels[labels == pad_id] = IGNORE_INDEX
    if pad_id < 0:
        input_ids[input_ids == pad_id] = unk_id

    if use_tp and get_model_parallel_world_size() > 1:
        # aggregate(concat) all the inputs across tp sync_group
        new_input_ids = torch.empty_like(input_ids).repeat(sync_group.size(), 1)
        new_labels = torch.empty_like(labels).repeat(sync_group.size(), 1)

        torch.distributed.all_gather_into_tensor(
            new_input_ids, input_ids, group=sync_group
        )
        torch.distributed.all_gather_into_tensor(new_labels, labels, group=sync_group)

        return new_input_ids, new_labels

    return input_ids, labels


if __name__ == "__main__":
    parser = HfArgumentParser((TrainingArguments,))
    args = parser.parse_args_into_dataclasses()[0]
    main(args)

```

å·¥ç¨‹åŒ–è¿˜è¯æ˜äº† training sequence length çš„é‡è¦æ€§, çººç»‡ cufoff, è®­ç»ƒé•¿ thinking å°±çŸ­:

Training sequence length ablation. We report â€œaccuracy / average thinking tokens per sampleâ€; the higher the accuracy and the fewer the thinking tokens(inference cost) the better.

|                           | Model A       | Model B      |
| :------------------------ | :------------ | :----------- |
| Training sequence length  | 4096          | 32768        |
| % training samples cutoff | 74%           | 0%           |
| AIME24                    | 30.0% / 20721 | 50.0% / 6984 |
| MATH500                   | 90.0% / 5324  | 91.0% / 3268 |
| GPQA                      | 52.5% / 6841  | 53.0% / 3568 |

(ç›´è§‚çš„ modelA/B çš„å¯¹æ¯” table çœ‹è®ºæ–‡ 20 é¡µ)

Besides our scaling ablations in Â§5.2, the main training hyperparameter we ablate is the sequence length used during training.(ä¸»è¦çš„è®­ç»ƒè¶…å‚æ•°æ¶ˆèç ”ç©¶æ˜¯è®­ç»ƒæœŸé—´ä½¿ç”¨çš„åºåˆ—é•¿åº¦) We find that a shorter training sequence length leads to longer reasoning traces at test time.(è¾ƒçŸ­çš„è®­ç»ƒåºåˆ—é•¿åº¦ä¼šå¯¼è‡´æµ‹è¯•æ—¶æ›´é•¿çš„æ¨ç†è½¨è¿¹) This is because when training with a shorter sequence length the answer section of the training sample is more commonly cut off.(è¿™æ˜¯å› ä¸ºä½¿ç”¨è¾ƒçŸ­åºåˆ—é•¿åº¦è®­ç»ƒæ—¶ï¼Œè®­ç»ƒæ ·æœ¬çš„ç­”æ¡ˆéƒ¨åˆ†æ›´å¸¸è¢«æˆªæ–­) Inversely, when the training sequence length is longer, more samples appear in their entirety with the section where the model answers. Thus the model receives more gradient updates where it learns to generate an answer following its chain.(ç›¸åï¼Œä½¿ç”¨è¾ƒé•¿çš„è®­ç»ƒåºåˆ—é•¿åº¦æ—¶ï¼Œæ›´å¤šæ ·æœ¬ä¼šå®Œæ•´åœ°å‡ºç°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒåŒ…æ‹¬æ¨¡å‹ä½œç­”çš„éƒ¨åˆ†ã€‚å› æ­¤ï¼Œæ¨¡å‹ä¼šæ¥æ”¶åˆ°æ›´å¤šçš„æ¢¯åº¦æ›´æ–°ï¼Œå­¦ä¹ åœ¨ç”Ÿæˆç­”æ¡ˆæ—¶éµå¾ªå…¶æ¨ç†é“¾) This in turn leads to a higher log probability of the answer section at any point during the generation and thus shorter reasoning traces at test time. Performance-wise, we also find that the model trained with a longer sequence length performs better. Thus we opt for the longest training sequence length as it leads to better performance and makes inference more efficient by leading to shorter reasoning traces.

ä¹Ÿå°±æ˜¯è¯´ DPO åå¥½æ•°æ®è®­ç»ƒä¼˜åŒ– ä¸æ˜¯å•¥å¥½æ–¹æ³•, ä¸åº”è¯¥å’Œ SFT ä¸€èµ·ç”¨?

stf.py:

```py
import os
import sys
from dataclasses import dataclass, field, asdict
from typing import Optional
import os
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
from datasets import load_dataset, concatenate_datasets, DatasetDict
import transformers
import trl

@dataclass
class TrainingConfig:
    model_name: str = field(default="Qwen/Qwen2.5-32B-Instruct")
    block_size: int = field(default=32768)
    wandb_project: Optional[str] = field(default="s1")
    wandb_entity: Optional[str] = field(default="hashimoto-group")
    train_file_path: Optional[str] = field(default='simplescaling/s1K_tokenized')
    dagger: bool = field(default=False)

    def __post_init__(self):
        os.environ['WANDB_PROJECT'] = self.wandb_project
        os.environ['WANDB_ENTITY'] = self.wandb_entity

def train():
    # parsing input
    parser = transformers.HfArgumentParser((TrainingConfig, trl.SFTConfig))
    config, args = parser.parse_args_into_dataclasses()
    log_config = {**asdict(config), **asdict(args)}
    logging.info(f"Training config: {log_config}")

    # loading model
    kwargs = {}
    if "70B" in config.model_name:
        # Removed "low_cpu_mem_usage": True, for 70B, since by default we are in FSDP,
        # it's more efficient to do  "cpu_ram_efficient_loading": true, in fsdp_config.json
        kwargs = {"device_map": "auto", "torch_dtype": "auto",
                  "attn_implementation": "flash_attention_2", "use_cache": False}
        model = transformers.AutoModelForCausalLM.from_pretrained(config.model_name, **kwargs)
    else:
        model = transformers.AutoModelForCausalLM.from_pretrained(config.model_name)

    dataset = load_dataset(config.train_file_path)

    # setting up trainer
    tokenizer = transformers.AutoTokenizer.from_pretrained(config.model_name, use_fast=True)
    if "Llama" in config.model_name:
        instruction_template = "<|start_header_id|>user<|end_header_id|>"
        response_template = "<|start_header_id|>assistant<|end_header_id|>\n\n"
        # Use a token that is never used
        tokenizer.pad_token = "<|reserved_special_token_5|>"
    elif "Qwen" in config.model_name:
        instruction_template = "<|im_start|>user"
        response_template = "<|im_start|>assistant\n"
        # Use a token that is never used
        tokenizer.pad_token = "<|fim_pad|>"

    # Only compute loss over assistant responses
    # Verified that it precisely starts where the thinking tokens start and ends with the first pad token
    # via labels being set to -100
    collator = trl.DataCollatorForCompletionOnlyLM(
        instruction_template=instruction_template,
        response_template=response_template,
        tokenizer=tokenizer,
        mlm=False
    )
    args.dataset_text_field = 'text'
    args.max_seq_length = config.block_size
    trainer = trl.SFTTrainer(
        model,
        train_dataset=dataset['train'],
        eval_dataset=dataset['test'] if 'test' in dataset else dataset['train'],
        args=args,
        data_collator=collator
    )

    trainer.train()
    trainer.save_model(output_dir=args.output_dir)
    tokenizer.save_pretrained(args.output_dir)
    trainer.accelerator.wait_for_everyone()


if __name__ == "__main__":
    train()

```

å®šä¹‰è®­ç»ƒé…ç½®(TrainingConfig)

ä½¿ç”¨@dataclass è£…é¥°å™¨å®šä¹‰äº†ä¸€ä¸ª TrainingConfig ç±»ï¼Œç”¨äºå­˜å‚¨å¾®è°ƒè¿‡ç¨‹æ‰€éœ€çš„é…ç½®å‚æ•°ï¼ŒåŒ…æ‹¬æ¨¡å‹åç§°ã€å—å¤§å°ã€WandB(Weights & Biases)é¡¹ç›®åç§°å’Œå®ä½“åç§°ã€è®­ç»ƒæ–‡ä»¶è·¯å¾„ã€æ˜¯å¦ä½¿ç”¨ Dagger ç­‰ã€‚
åœ¨`post_init`æ–¹æ³•ä¸­ï¼Œå°†é…ç½®ä¸­çš„ wandb_project å’Œ wandb_entity è®¾ç½®ä¸ºç¯å¢ƒå˜é‡ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸å…¶ä»–åº“æˆ–å·¥å…·é›†æˆã€‚

å®šä¹‰è®­ç»ƒå‡½æ•°(train)

ä½¿ç”¨ HfArgumentParser è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå°†å‚æ•°è§£æä¸º TrainingConfig å’Œ trl.SFTConfig ä¸¤ä¸ªæ•°æ®ç±»å¯¹è±¡ã€‚
å°†è§£æå‡ºçš„é…ç½®å‚æ•°åˆå¹¶ä¸ºä¸€ä¸ªå­—å…¸ï¼Œå¹¶è®°å½•åˆ°æ—¥å¿—ä¸­ã€‚
æ ¹æ®é…ç½®ä¸­çš„æ¨¡å‹åç§°åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚å¦‚æœæ¨¡å‹åç§°åŒ…å«"70B"ï¼Œåˆ™ä½¿ç”¨ç‰¹å®šçš„å‚æ•°æ¥æé«˜å¤§æ¨¡å‹çš„åŠ è½½æ•ˆç‡ã€‚
ä½¿ç”¨ load_dataset å‡½æ•°ä»é…ç½®ä¸­æŒ‡å®šçš„è·¯å¾„åŠ è½½è®­ç»ƒæ•°æ®é›†ã€‚
æ ¹æ®æ¨¡å‹åç§°è®¾ç½®ç‰¹å®šçš„æŒ‡ä»¤æ¨¡æ¿(instruction_template)å’Œå“åº”æ¨¡æ¿(response_template)ï¼Œå¹¶ä¸º tokenizer è®¾ç½®å¡«å……ç¬¦(pad_token)ã€‚
åˆ›å»ºä¸€ä¸ª DataCollatorForCompletionOnlyLM å¯¹è±¡ï¼Œç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‡†å¤‡æ•°æ®æ‰¹æ¬¡ã€‚åªæœ‰åœ¨åŠ©æ‰‹(assistant)å“åº”çš„éƒ¨åˆ†è®¡ç®—æŸå¤±(loss)ï¼Œå¹¶ä¸”é€šè¿‡è®¾ç½®æ ‡ç­¾(labels)ä¸º-100 æ¥å¿½ç•¥å…¶ä»–éƒ¨åˆ†ã€‚
åˆ›å»º SFTTrainer å¯¹è±¡ï¼Œç”¨äºå¾®è°ƒè®­ç»ƒè¿‡ç¨‹ã€‚ä¼ å…¥æ¨¡å‹ã€è®­ç»ƒæ•°æ®é›†ã€è¯„ä¼°æ•°æ®é›†ã€å¾®è°ƒå‚æ•°å’Œæ•°æ®æ•´ç†å™¨(collator)ã€‚
è°ƒç”¨ trainer.train()å¼€å§‹å¾®è°ƒè®­ç»ƒã€‚
è®­ç»ƒå®Œæˆåï¼Œå°†å¾®è°ƒåçš„æ¨¡å‹å’Œåˆ†è¯å™¨(tokenizer)ä¿å­˜åˆ°æŒ‡å®šçš„è¾“å‡ºç›®å½•

```py
    # setting up trainer
    tokenizer = transformers.AutoTokenizer.from_pretrained(config.model_name, use_fast=True)
    if "Llama" in config.model_name:
        instruction_template = "<|start_header_id|>user<|end_header_id|>"
        response_template = "<|start_header_id|>assistant<|end_header_id|>\n\n"
        # Use a token that is never used
        tokenizer.pad_token = "<|reserved_special_token_5|>"
    elif "Qwen" in config.model_name:
        instruction_template = "<|im_start|>user"
        response_template = "<|im_start|>assistant\n"
        # Use a token that is never used
        tokenizer.pad_token = "<|fim_pad|>"
```

è¿™é‡Œæ˜¯å› ä¸º pad_token ä¸ä¼šä¸è®­ç»ƒæ•°æ®ä¸­çš„ä»»ä½•å®é™…æ–‡æœ¬å†²çªã€‚ä¸åŒçš„æ¨¡å‹å¯èƒ½ä½¿ç”¨ä¸åŒçš„ç‰¹æ®Šæ ‡è®°ç¬¦(special tokens)ï¼Œå› æ­¤ä¸ºäº†ç¡®ä¿å¡«å……ç¬¦ä¸ä¸è¿™äº›æ ‡è®°ç¬¦å†²çª

```sh
uid="$(date +%Y%m%d_%H%M%S)"
base_model="Qwen/Qwen2.5-32B-Instruct"
lr=1e-5
min_lr=0
epochs=5
micro_batch_size=2 # -> batch_size will be 16 if 8 gpus
push_to_hub=true
gradient_accumulation_steps=2
max_steps=-1
gpu_count=$(nvidia-smi -L | wc -l)

torchrun --nproc-per-node ${gpu_count} --master_port 12345 \
train/sft.py \
--per_device_train_batch_size=${micro_batch_size} \
--per_device_eval_batch_size=${micro_batch_size} \
--gradient_accumulation_steps=${gradient_accumulation_steps} \
--num_train_epochs=${epochs} \
--max_steps=${max_steps} \
--train_file_path="simplescaling/s1K_tokenized" \
--model_name=${base_model} \
--warmup_ratio=0.05 \
--fsdp="full_shard auto_wrap" \
--fsdp_config="train/fsdp_config_qwen.json" \
--bf16=True \
--eval_strategy="steps" \
--eval_steps=50 \
--logging_steps=1 \
--save_steps=100 \
--lr_scheduler_type="cosine" \
--learning_rate=${lr} \
--weight_decay=1e-4 \
--adam_beta1=0.9 \
--adam_beta2=0.95 \
--output_dir="ckpts/s1_${uid}" \
--hub_model_id="simplescaling/s1-${uid}" \
--push_to_hub=${push_to_hub} \
--save_only_model=True
```

è¿™é‡Œè¦å¤ä¹ ä¸‹ fsdp:

FSDPï¼Œå³å…¨åˆ‡ç‰‡æ•°æ®å¹¶è¡Œï¼Œæ˜¯ä¸€ç§å°†æ•°æ®å¹¶è¡Œç­–ç•¥æ¨å‘æè‡´çš„æŠ€æœ¯ã€‚ ä¸ä¼ ç»Ÿçš„æ•°æ®å¹¶è¡Œ(DDP)ä¸åŒï¼ŒFSDP ä¸ä»…å°†æ•°æ®é›†åˆ‡åˆ†ä¸ºå¤šä¸ªåˆ†ç‰‡ç»™ä¸åŒçš„ GPU è¿›è¡Œè®­ç»ƒï¼Œè¿˜å°†æ¨¡å‹çš„å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€å’Œæ¢¯åº¦éƒ½è¿›è¡Œäº†åˆ†ç‰‡ã€‚ è¿™æ ·ï¼Œæ¯ä¸ª GPU åªéœ€ä¿å­˜æ¨¡å‹çš„ä¸€éƒ¨åˆ†å‚æ•°ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†å•ä¸ª GPU çš„å†…å­˜å ç”¨ï¼Œä½¿å¾—è®­ç»ƒæ›´å¤§è§„æ¨¡çš„æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚

fsdpï¼šä¸€ä¸ªå¸ƒå°”å€¼æˆ–å­—ç¬¦ä¸²æˆ–å…³äº FSDPOption çš„åˆ—è¡¨ï¼ŒæŒ‡å®šä½¿ç”¨ PyTorch Distributed Parallel Training è®­ç»ƒ(ä»…åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­)ã€‚å¯é€‰çš„å‚æ•°ä¸ºï¼š

"full_shard"ï¼šå¯¹ parameters, gradients, optimizer states è¿›è¡Œåˆ†ç‰‡ã€‚
"shard_grad_op"ï¼šå¯¹ optimizer states, gradients è¿›è¡Œåˆ†ç‰‡ã€‚
"offload"ï¼šå°† parameters, gradients å¸è½½åˆ° CPU(ä»…ä¸ "full_shard", "shard_grad_op" å…¼å®¹)ã€‚
"auto_wrap"ï¼šä½¿ç”¨ default_auto_wrap_policy ä»è€Œåˆ©ç”¨ FSDP æ¥è‡ªåŠ¨é€’å½’åœ° wrap layers ã€‚
fsdp_min_num_paramsï¼šä¸€ä¸ªæ•´æ•°ï¼ŒæŒ‡å®š FSDP çš„é»˜è®¤çš„æœ€å°‘çš„ parameters æ•°é‡ä»è€Œç”¨äº Default Auto Wrapping ã€‚ä»…å½“ fsdp å‚æ•°æœ‰æ•ˆæ—¶ fsdp_min_num_params æ‰æœ‰æ„ä¹‰ã€‚

@see: https://www.huaxiaozhuan.com/%E5%B7%A5%E5%85%B7/huggingface_transformer/chapters/4_trainer.html

fp16 å’Œ bf16 åŒºåˆ«:

FP16 å¯è¡¨ç¤ºåè¿›åˆ¶ä¸‹çš„ä¸‰ä½æœ‰æ•ˆæ•°å­—æ ¼å¼(log21110âˆ¼3.311) ï¼ŒBF16 å¯è¡¨ç¤ºåè¿›åˆ¶ä¸‹çš„ 2 ä½æœ‰æ•ˆæ•°å­—æ ¼å¼(log2810âˆ¼2.408)ï¼Œä½†æŒ‰ç…§æµ®ç‚¹æ•°çš„æ ¼å¼ï¼Œå…¶è¡¨ç¤ºçš„ç»å¯¹ç²¾åº¦(ç²¾ç¡®åˆ°å“ªä¸€æ•°ä½)åœ¨å„ä¸ªåŒºé—´(æœ¬è´¨ä¸ºå„ä¸ªé˜¶ç ä¸‹å¹¶ä¸ç›¸åŒ)ã€‚

BF16 æ˜¯å¯¹ FP32 å•ç²¾åº¦æµ®ç‚¹æ•°æˆªæ–­æ•°æ®ï¼Œå³ç”¨ 8bit è¡¨ç¤ºæŒ‡æ•°ï¼Œ7bit è¡¨ç¤ºå°æ•°

FP16 åŠç²¾åº¦æµ®ç‚¹æ•°ï¼Œç”¨ 5bit è¡¨ç¤ºæŒ‡æ•°ï¼Œ10bit è¡¨ç¤ºå°æ•°\

ä¸ 32 ä½ç›¸æ¯”ï¼Œé‡‡ç”¨ BF16/FP16 ååé‡å¯ä»¥ç¿»å€ï¼Œå†…å­˜éœ€æ±‚å¯ä»¥å‡åŠã€‚ä½†æ˜¯è¿™ä¸¤è€…ç²¾åº¦ä¸Šå·®å¼‚ä¸ä¸€æ ·ï¼ŒBF16 å¯è¡¨ç¤ºçš„æ•´æ•°èŒƒå›´æ›´å¹¿æ³›ï¼Œä½†æ˜¯å°¾æ•°ç²¾åº¦è¾ƒå°ï¼›FP16 è¡¨ç¤ºæ•´æ•°èŒƒå›´è¾ƒå°ï¼Œä½†æ˜¯å°¾æ•°ç²¾åº¦è¾ƒé«˜ã€‚

@see: https://zhuanlan.zhihu.com/p/449345588

å› ä¸º s1 ç”¨çš„æ˜¯ 2 ä¸ªåˆ«äººçš„æ¨¡å‹ api åšæ•°æ®ç­›é€‰å·¥ä½œ, ä¸»è¥ä¸šåŠ¡æ˜¯ SFT å’Œæ¨ç†, æ‰€ä»¥ç”¨çš„ BF16?

ä½†ä¸ºå•¥ä¸ç”¨ FP8 å‘¢? å·²ç»è¯æ˜äº† FP8 å†…å­˜å ç”¨å°‘äº BF16 42%, é€Ÿåº¦å¿« 64%

@see: https://www.jiqizhixin.com/articles/2023-11-02-10

@see: https://arxiv.org/abs/2310.18313

@see: https://github.com/Azure/MS-AMP

æ— æ³•è§£é‡Š
