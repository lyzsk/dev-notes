# Simple test-time scaling

@see: https://arxiv.org/abs/2501.19393

亮点: test-time compute 就是多花时间在 resoning 的过程, 结合了低预算

只用了 SFT 没用 RL(Reinforcement learning), deepseek r1 用的是 RL 进行推理

推理数据少, 推理数据标注难, 比较适合 RL, 做趋势学习, 而非明确的目标

所以只用 SFT 就是说明要精心挑选数据(只用了 1000 samles)(We show that training on only 1,000 samples with next-token prediction and controlling thinking duration via a simple test-time technique we refer to as budget forcing leads to a strong reasoning model that scales in performance with more test-time compute.)

如何从 50K 数据里选择 1000 条, 筛选数据经验

极限压低预算, 训练成本 26min 16H100 1000 samples(用 Google Gemini Flash Thinking API 生成), e.g. 推理过程用 Gemini 生成 reasoning trace and response, 帮忙合成数据, 用 Claude 3.5 Sonnet 进行数据分类. 就是用其他模型的能力帮忙处理数据, 比如 deepseek 蒸馏, 这种互相交互数据, 但是蒸馏就面临 teacher 模型需要 logic, 像 GPT 这种闭源无源代码的就没办法用了

1. Recently, OpenAI’s o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. 就是推理过程不知道怎么实现
2. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. 探索推理过程中的增强来提升训练能力, 就是 59K->1k samples, criteria 就是 difficulty, diversity, quality
3. Second, we develop budget forcing to control test-time compute by forcefully terminating the model’s thinking process or lengthening it by appending “Wait” multiple times to the model’s generation when it tries to end. 就是用 `budget forcing` 这个方法来控制是否继续推理还是给出 final answer, 其实就是 max token, 数量到极值就停止 thinking

以前是 multi agent approach, 多个模型去散布数据, 现在发现不需要了, 只要 SFT 了

怎样让一个模型去思考, 让它开始的时候不回答问题, 直接生成一个 context, 先 thinking, 然后到 max token 再 answer, 也就是先是生成(context+thinkg) \* n, 最后 select, 就完成了推理

We collect an initial 59,029 questions from 16 diverse sources following three guiding principles: **Quality**, where datasets should be of high quality—we always inspect samples and ignore datasets with issues such as poor formatting; **Difficulty**, where datasets should be challenging and require significant reasoning effort; and **Diversity**, where datasets should stem from different fields to cover a wide range of reasoning tasks. We collect datasets of two categories:

-   Curation of existing datasets(主要是 online websites 数学问题), 然后在基础上为了提升 diversity, 添加了 Questions spanning Astronomy, Biology, Chemistry, Computer Science, Geography, Mathematics, and Physics are collected from various Olympiads, ensuring a diverse and challenging set of problems that require advanced reasoning and problem-solving skills. These questions are designed to test deep understanding and application of knowledge across multiple disciplines, making them ideal for generating exercises that push the boundaries of critical thinking and analytical abilities. 然后还加了 covering English, Law, and Logic 的 AGIEval.

-   New datasets in quantitative reasoning, **s1-prob** consists of 182 questions from the probability section of Stanford University’s Statistics Department’s PhD Qualifying Exams, accompanied by handwritten solutions that cover difficult proofs(涵盖困难的证明还要是手写的, 也就是不太好格式化的). The probability qualifying exam is held yearly and requires professional-level mathematical problem-solving, making it an excellent resource for advanced learners. Complementing this, **s1-teasers** comprises 23 challenging brain-teasers(脑经急转弯) commonly used in interview questions for quantitative trading positions(量化交易头寸), which test logical reasoning and creative problem-solving skills. Together, these datasets provide a comprehensive range of problems that span rigorous academic challenges and real-world applications, ensuring a well-rounded preparation for both theoretical and practical problem-solving scenarios. We only take examples with the highest difficulty level("Hard"). 说白了跟竞赛数据差不多?

参考上文说的:(I) If the model generates more thinking tokens than a desired limit, we forcefully end the thinking process by appending an end-of-thinking token delimiter. This approach ensures that the model transitions to generating its answer, preventing excessive or unnecessary reasoning.(II) Conversely, if we want the model to spend more test-time compute on a problem, we suppress the generation of the end-of-thinking token delimiter and instead append “Wait” to the model’s current reasoning trace. This encourages further exploration and deeper reasoning, allowing the model to potentially arrive at a more refined or accurate solution. These two strategies provide flexible control over the model’s reasoning process, balancing efficiency and thoroughness based on the specific requirements of the task.

也就是很多 token 是需要 delimiter 的

还有就是一些比较基础的问题, 调用的模型可以回答的出来就可以筛除了

Claude 主要是 diversity 每个领域划分 1 条数据

然后最重要的就是这些数据整合是靠接下来后文说的:

For each question, we generate a reasoning trace and solution using the Google Gemini Flash Thinking API(Google, 2024), extracting its reasoning trace and response. This process yields 59,000 triplets, each consisting of a question, a generated reasoning trace, and a generated solution. Examples from our dataset are detailed in §C.2. To ensure the integrity and quality of the data, we decontaminate all samples against our evaluation questions—MATH500, GPQA Diamond, and AIME24—using 8-grams, and we further deduplicate the data to remove any redundant or overlapping entries. This rigorous preprocessing step ensures that the dataset is clean, unique, and well-suited for training and evaluation purposes.

中间一步步筛选数据

**Quality** We first remove any questions where we ran into any API errors reducing our dataset to 54,116 samples. Next, we filter out low-quality examples by checking if they contain any string patterns with formatting issues, such as ASCII art diagrams, non-existent image references, or inconsistent question numbering reducing our dataset to 51,581 examples. From this pool, we identify 384 samples for our final 1,000 samples from datasets that we perceive as high-quality and not in need of further filtering(see §B.4 for details).

这里很主观, 可能因为用到了 online 数据, 过期不存在等问题, 还有格式化的标准型问题?

**Difficulty** For difficulty, we use two indicators: model performance and reasoning trace length(模型性能和推理轨迹长度). We evaluate two models on each question: Qwen2.5-7B-Instruct and Qwen2.5-32B-Instruct(Qwen et al., 2024), with correctness assessed by Claude 3.5 Sonnet comparing each attempt against the reference solution(see §B.3 for the grading protocol). We measure the token length of each reasoning trace to indicate problem difficulty using the Qwen2.5 tokenizer(Qwen2.5 分词器测量每个推理轨迹的标记长度 以表示问题的难度). This relies on the assumption that more difficult problems require more thinking tokens. Based on the grading, we remove questions that either Qwen2.5-7B-Instruct or Qwen2.5-32B-Instruct can solve correctly and thus may be too easy(也就是移出 Qwen 能正确解决的问题, 因为可能过于简单). By using two models we reduce the likelihood of an easy sample slipping through our filtering due to a rare mistake on an easy question of one of the models. This brings our total samples down to 24,496(也就是用了 2 个现有模型也才减半), setting the stage for the next round of subsampling based on diversity. While filtering with these two models may be optimized for our setup as we will also use Qwen2.5-32B-Instruct as our model to finetune, the idea of model-based filtering generalizes to other setups.

**Diversity** To quantify diversity we classify each question into specific domains using Claude 3.5 Sonnet based on the Mathematics Subject Classification(MSC) system(e.g., geometry, dynamic systems, real analysis, etc.)(几何、动态系统、实分析等) from the American Mathematical Society.1 The taxonomy focuses on topics in mathematics but also includes other sciences such as biology, physics, and economics. To select our final examples from the pool of 24,496 questions, we first choose one domain uniformly at random.(首先均匀随机地选择一个领域) Then, we sample one problem from this domain according to a distribution that favors longer reasoning traces(see §B.4 for details) as motivated in Difficulty. We repeat this process until we have 1,000 total samples.(根据一个倾向于更长推理轨迹的分布, 从该领域中采样一个问题，如难度部分所述, 也就是这里又用了一次上面 2 个模型, 然后反复利用缩减至 1000)

benchmark 看 比 o1-preview 强, 比 o1 差, 够了, 因为节省了资源, 还是 32B 的

工程化部分:

We take a model that has already been pretrained and instruction tuned and further finetune it for reasoning.(选取一个已经预训练和指令调优的模型, 并进一步对其进行推理调优) Specifically, we use Qwen2.5-32B-Instruct(Qwen et al., 2024), which on math tasks generally matches or outperforms the larger Qwen2.5-72B-Instruct(Qwen et al., 2024) or other open models(Dubey et al., 2024; Groeneveld et al., 2024; Muennighoff et al., 2024). We use token delimiters to separate the thinking stage from the answering stage.(使用标记分隔符来区分思考阶段和回答阶段) We enclose the thinking stage with <| im_start |>think and <| im_start |>answer; both preceded and followed by a newline. Samples from our dataset are in §C.2. We use basic fine-tuning hyperparameters: we train for 5 epochs with a batch size of 16 for a total of 315 gradient steps.(5 epochs, 16 batch size, 315 gradient steps 可以说是非常非常牛逼了) We train in bfloat16 precision with a learning rate of 1𝑒 − 5 warmed up linearly for 5%(16 steps) and then decayed to 0 over the rest of training(299 steps) following a cosine schedule. We use the AdamW optimizer(Loshchilov & Hutter, 2019) with 𝛽1 = 0.9, 𝛽2 = 0.95 and weight decay of 1𝑒 − 4.(学习率开始值 1e-5 用 Adam optimizer 前 16step 5% 线性预热, 后 299step 根据余弦调度衰减至 0, 学习率居然还可以衰减到 0?!) We do not compute loss on questions, only on reasoning traces and solutions. We ensure the sequence length is large enough to avoid cutting off any samples; a setting we ablate in §C.1. The training takes just 26 minutes on 16 NVIDIA H100 GPUs.

补充: 两个向量余弦相似度是负数, 所以才能衰减

@see: https://juejin.cn/post/7434419051331649573

@see: https://cloud.tencent.com/developer/information/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%A4%E4%B8%AA%E5%90%91%E9%87%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E6%80%A7%E6%98%AF%E8%B4%9F%E7%9A%84%EF%BC%9F

标记代码:

```py
"""Compute avg thinking steps"""

import sys
import json

path = sys.argv[1] # e.g. qwen_20241129-012159-32steps_32kctxt/ckpts__qwen_20241129-012159/samples_openai_math_2024-12-01T01-56-47.156277.jsonl
try:
    # allowed_steps = int(path.split('steps')[0].split('-')[-1].split('_')[-1])
    allowed_steps = int(path.split('step')[-1].split('/')[0].split('forcing')[0])
    allowed_tokens = None
except:
    try:
        allowed_tokens = int(path.split('tokens')[0].split('-')[-1].split('_')[-1])
        allowed_steps = None
    except:
        allowed_steps, allowed_tokens = None, None
        print("No steps/tokens in path; Assuming it was run without steps/tokens limit")

tokens = True
total_steps = 0
step_lens = []
total_lens = []
total_lens_with_sep = []
total_lens_answer = []
samples = 0
samples_too_many_steps = 0
samples_too_many_tokens = 0
samples_too_many_thinking_tokens = 0
samples_without_answer = 0
#samples_without_answer_but_steps = 0
samples_without_thinking = 0
correct = []

if tokens:
    from transformers import AutoTokenizer
    tok = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-32B-Instruct")

with open(path) as f: # JSONL file
    for line in f:
        data = json.loads(line)

        if isinstance(data['filtered_resps'][0], list):
            data['filtered_resps'][0] = data['filtered_resps'][0][0]

        ### <|im_start|>step format ###
        # # total_steps += data['filtered_resps'][0].count('<|reserved_special_token_2|>')
        # total_steps += data['filtered_resps'][0].count('<|im_start|>step')
        # samples_without_answer += int('<|im_start|>answer' not in data['filtered_resps'][0])
        # #samples_without_answer_but_steps += int(('<|im_start|>answer' not in data['filtered_resps'][0]) and('<|im_start|>step' in data['filtered_resps'][0]))
        # samples_without_thinking += int('<|im_start|>step' not in data['filtered_resps'][0])
        # samples_too_many_steps += int(data['filtered_resps'][0].count('<|im_start|>step') > allowed_steps)
        # step_lens.extend([len("\n".join(step.split("\n")[1:])) for step in data['filtered_resps'][0].split('<|im_start|>step')[1:-1]])
        # samples += 1

        if tokens and("qwq" in path):
            total_lens_with_sep.append(len(tok.tokenize(data['filtered_resps'][0])))

            too_long = int(len(tok.tokenize(data['arguments']['gen_args_0']['arg_0'] + data['filtered_resps'][0])) > 32760)
            samples_too_many_tokens += too_long

            no_answer = int(('Answer:' not in data['filtered_resps'][0]) and('\\boxed' not in data['filtered_resps'][0]))
            samples_without_answer += no_answer
        elif tokens and allowed_steps:
            total_steps += data['filtered_resps'][0].count(' steps left\n')
            samples_without_thinking += int(' steps left\n' not in data['filtered_resps'][0])
            samples_too_many_steps += int(data['filtered_resps'][0].count(' steps left\n') > allowed_steps)
            # if int(data['filtered_resps'][0].count(' steps left\n') > allowed_steps):
            #     import pdb; pdb.set_trace()
            step_lens_tmp = [len(tok.tokenize("\n".join(step.split("\n")[1:]))) for step in data['filtered_resps'][0].split('<|im_start|>')[1:-1]]
            total_lens.extend([sum(step_lens_tmp)])
            total_lens_with_sep.extend([len(tok.tokenize(data['filtered_resps'][0]))])
            if "<|im_start|>answer" in data['filtered_resps'][0]:
                total_lens_answer.append(len(tok.tokenize(data['filtered_resps'][0].split('<|im_start|>answer')[1])))
            else:
                total_lens_answer.append(0)
            step_lens.extend(step_lens_tmp)

            #import pdb; pdb.set_trace()
            too_long = int(len(tok.tokenize(data['arguments']['gen_args_0']['arg_0'] + data['filtered_resps'][0])) > 32760) # 32768
            samples_too_many_tokens += too_long

            no_answer = int(('Answer:' not in data['filtered_resps'][0]) and('\\boxed' not in data['filtered_resps'][0]))
            samples_without_answer += no_answer

            # Worth checking
            #if no_answer and not(too_long):
            #    import pdb; pdb.set_trace()
        elif tokens:
            steps = data['filtered_resps'][0].split('<|im_start|>answer')[0].split("\n")[1:-1]
            thinking = "\n".join(steps)
            if '<|im_start|>answer' in data['filtered_resps'][0]:
                answer = data['filtered_resps'][0].split('<|im_start|>answer')[-1]
                if "\n" in answer:
                    answer = "\n".join(answer.split("\n")[1:])
            else:
                answer = ""

            total_steps += data['filtered_resps'][0].split('<|im_start|>answer')[0].count('\n') - 1
            samples_without_thinking += int('<|im_start|>think' not in data['filtered_resps'][0])

            step_lens_tmp = [len(tok.tokenize(step)) for step in steps]
            step_lens.extend(step_lens_tmp)

            thinking_tokens = len(tok.tokenize(thinking))
            total_lens.append(thinking_tokens)

            #if sum(step_lens_tmp) == 30119:
            #   import pdb; pdb.set_trace()

            if allowed_tokens:
                samples_too_many_thinking_tokens += int(thinking_tokens > allowed_tokens)

            total_lens_with_sep.append(len(tok.tokenize(data['filtered_resps'][0])))
            total_lens_answer.append(len(tok.tokenize(answer)))

            too_long = int(len(tok.tokenize(data['arguments']['gen_args_0']['arg_0'] + data['filtered_resps'][0])) > 32760)
            if too_long:
                import pdb; pdb.set_trace()
            samples_too_many_tokens += too_long

            no_answer = int(('Answer:' not in data['filtered_resps'][0]) and('\\boxed' not in data['filtered_resps'][0]))
            samples_without_answer += no_answer

            # Worth checking
            #if not(no_answer) and too_long:
            #    import pdb; pdb.set_trace()
            #if no_answer and not(too_long):
            #    import pdb; pdb.set_trace()
            # if no_answer:
            #    import pdb; pdb.set_trace()

        ### <|im_start|>13 steps left format ###
        else:
            if allowed_steps:
                total_steps += data['filtered_resps'][0].count(' steps left\n')
                samples_without_thinking += int(' steps left\n' not in data['filtered_resps'][0])
                samples_too_many_steps += int(data['filtered_resps'][0].count(' steps left\n') > allowed_steps)
                step_lens_tmp = [len("\n".join(step.split("\n")[1:])) for step in data['filtered_resps'][0].split('<|im_start|>')[1:-1]]
                total_lens.extend([sum(step_lens_tmp)])
                step_lens.extend(step_lens_tmp)
            else:
                # Approximation
                total_steps += data['filtered_resps'][0].split('<|im_start|>answering\n')[0].count('\n') - 1
                samples_without_thinking += int('<|im_start|>thinking\n' not in data['filtered_resps'][0])
                step_lens.extend([len("\n".join(step.split("\n")[1:])) for step in data['filtered_resps'][0].split('<|im_start|>answering\n')[0].split('\n')[1:-1]])
                total_lens.extend([len(data['filtered_resps'][0].split('<|im_start|>answering\n')[0])])

            samples_without_answer += int('<|im_start|>answer' not in data['filtered_resps'][0])
            #samples_without_answer_but_steps += int(('<|im_start|>answer' not in data['filtered_resps'][0]) and('<|im_start|>step' in data['filtered_resps'][0]))

        samples += 1
        correct.append(data['exact_match'])

print("# samples:", samples)
if tokens:
    print("avg steps:", round(total_steps / samples, 1))
    if step_lens:
        print("avg step tokens:", round(sum(step_lens) / len(step_lens), 1))
    if total_lens:
        print("min total thinking tokens:", min(total_lens))
        print("max total thinking tokens:", max(total_lens))
        print("sorted total thinking tokens:", sorted(total_lens))
        correct_sorted = [correct[i] for i in sorted(range(len(total_lens)), key=lambda k: total_lens[k])]
        print("sorted correct:", correct_sorted)
        print("avg total thinking tokens:", round(sum(total_lens) / len(total_lens), 1))
    if total_lens_answer:
        print("avg total answer tokens:", round(sum(total_lens_answer) / len(total_lens_answer), 1))
    print("avg total tokens:", round(sum(total_lens_with_sep) / len(total_lens_with_sep), 1))
    if allowed_steps:
        print("samples w/ too many steps:", samples_too_many_steps)
    else:
        print("samples w/ too many thinking tokens:", samples_too_many_thinking_tokens)
    print("samples w/ too many tokens:", samples_too_many_tokens)
    print("samples w/o answer:", samples_without_answer)
    print("samples w/o thinking:", samples_without_thinking)
elif allowed_steps:
    print("avg steps:", round(total_steps / samples, 1))
    print("avg step len:", round(sum(step_lens) / len(step_lens), 1))
    print("avg total thinking len:", round(sum(total_lens) / len(total_lens), 1))
    print("samples w/ too many steps:", samples_too_many_steps)
    print("samples w/o 'Answer:':", samples_without_answer)
    #print("samples w/o answer but steps:", samples_without_answer_but_steps)
    print("samples w/o thinking:", samples_without_thinking)
else:
    print("avg steps(approx via \\n):", round(total_steps / samples, 1))
    print("avg step len(approx via \\n):", round(sum(step_lens) / len(step_lens), 1))
    print("avg total thinking len:", round(sum(total_lens) / len(total_lens), 1))
    print("samples w/o answer:", samples_without_answer)
    print("samples w/o thinking:", samples_without_thinking)

```

训练 epoch, step, lr, etc. 代码:

```py
# Copyright(c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
import os
import sys
import tempfile
import time
from pathlib import Path
import itertools

import tqdm

import torch
import torch.nn.functional as F
from torch.utils.data.distributed import DistributedSampler

import torch._inductor.config
import torch._dynamo.config

torch._inductor.config.coordinate_descent_tuning = True
torch._inductor.config.triton.unique_kernel_names = True
torch._inductor.config.fx_graph_cache = True  # Experimental feature to reduce compilation times, will be on by default in future

torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

try:
    import wandb
except ImportError:
    wandb = None

# support running without installing as a package
wd = Path(__file__).parent.parent.resolve()
sys.path.append(str(wd))

from models.model import Transformer, set_global_compile_mode
from models.tp import(
    maybe_init_dist,
    initialize_model_parallel,
    get_model_parallel_group,
    get_model_parallel_world_size,
    get_data_parallel_world_size,
    clip_grad_norm_,
    compute_vocab_parallel_logprobs,
)

from data_utils.common_utils import manual_seed
from data_utils.data_utils_sft import make_sft_data_module
from data_utils.tokenizer_utils import FakePreTrainedTokenizer

from training_utils.hf_argparser import HfArgumentParser
from training_utils.training_args import TrainingArguments
from training_utils.checkpoint_hook import(
    checkpoint_hook,
    get_latest_checkpoint_path,
    load_checkpoint,
    load_model_from_from_ckpt,
)
from training_utils.trainer_utils import(
    create_optimizer,
    create_fsdp_model_for_finetune,
    get_cosine_schedule_with_warmup,
)

IGNORE_INDEX = -100


def model_forward(model, x, input_pos):
    return model(x, input_pos, fully_causal=True)


def model_forward_with_loss(
    model: Transformer,
    input_ids: torch.Tensor,
    labels: torch.Tensor,
) -> torch.Tensor:
    """
    Compute the loss for a given model and prompts.
    """
    # create an empty tensor of the expected final shape and fill in the current tokens
    batch_size, T = input_ids.size(0), input_ids.size(1)

    device = input_ids.device
    with torch.device(device):
        model.setup_caches(max_batch_size=batch_size, max_seq_length=T, kv_cache=False)
    # create an empty tensor of the expected final shape and fill in the current tokens
    input_pos = torch.arange(0, T, device=device)

    with torch.backends.cuda.sdp_kernel(
        enable_flash=True, enable_math=False, enable_mem_efficient=False
    ):
        logits = model_forward(model, input_ids, input_pos)

    with torch.autocast(device_type="cuda", enabled=False):
        logits = logits.float()
        logits = logits[..., :-1, :].contiguous()
        labels = labels[..., 1:].contiguous()

        if model.vocab_parallel:
            loss = -compute_vocab_parallel_logprobs(
                logits.view(-1, logits.size(-1)),
                labels.view(-1),
                ignore_index=IGNORE_INDEX,
                reduction="mean",
            )
        else:
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                labels.view(-1),
                reduction="mean",
            )
    return loss


def encode_tokens(tokenizer, string, bos=True, device="cuda"):
    tokens = tokenizer.encode(string)
    if bos:
        tokens = [tokenizer.bos_id()] + tokens
    return torch.tensor(tokens, dtype=torch.int, device=device)


def main(
    args: TrainingArguments,
) -> None:
    """Finetune a model on a given dataset."""
    checkpoint_path = args.checkpoint_path
    sft_checkpoint_path = args.sft_checkpoint_path
    compile = args.compile
    assert checkpoint_path.is_file(), checkpoint_path

    if sft_checkpoint_path is not None:
        assert sft_checkpoint_path.is_dir(), sft_checkpoint_path

    tokenizer_path = checkpoint_path.parent / "tokenizer.model"
    if not tokenizer_path.is_file():
        tokenizer_path = checkpoint_path.parent

    set_global_compile_mode(compile)

    global print
    device_id = maybe_init_dist()
    use_tp = device_id is not None
    if use_tp:
        group_size = args.tensor_parallel_size or torch.distributed.get_world_size()
        initialize_model_parallel(group_size)
        torch.distributed.barrier()
        intra_node_group = get_model_parallel_group()

        if device_id != 0:
            # only print on rank 0
            print = lambda *args, **kwargs: None

    if args.report_to == "wandb" and wandb is not None:
        if device_id == 0:
            wandb_logging_dir = os.path.join(
                tempfile.gettempdir(), f"{os.getuid()}_wandb"
            )
            if not os.path.exists(wandb_logging_dir):
                os.makedirs(wandb_logging_dir, exist_ok=True)
            os.environ["WANDB_DIR"] = wandb_logging_dir
            wandb.init(
                name=args.wandb_name,
                project=args.wandb_project,
                entity=args.wandb_entity,
                resume="allow",
                magic=True,
                dir=wandb_logging_dir,
                force=True,
            )
            wandb.config.update(vars(args))

    device = "cuda"
    precision = args.param_dtype

    print("Loading model ...")
    t0 = time.time()
    model = load_model_from_from_ckpt(
        checkpoint_path,
        sft_checkpoint_path,
        device,
        precision,
        use_tp,
        requires_grad=True,
        sequence_parallel=args.sequence_parallel,
        vocab_parallel=args.vocab_parallel,
    )

    torch.cuda.synchronize()
    print(f"Time to load model: {time.time() - t0:.02f} seconds")

    tokenizer = FakePreTrainedTokenizer(tokenizer_path)

    data_module = make_sft_data_module(
        tokenizer=tokenizer,
        args=args,
    )
    train_dataset = data_module["train_dataset"]
    data_collator = data_module["data_collator"]

    model_size = sum(
        [
            p.numel() * p.dtype.itemsize
            for p in itertools.chain(model.parameters(), model.buffers())
        ]
    )

    print(f"Model size: {model_size / 1e6:.02f} MB")
    manual_seed(args.seed)

    sampler = None
    if use_tp:
        sampler = DistributedSampler(
            train_dataset,
            shuffle=True,
            drop_last=True,
        )

    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=args.per_device_train_batch_size,
        shuffle=(sampler is None),
        sampler=sampler,
        num_workers=0,
        pin_memory=True,
        collate_fn=data_collator,
    )

    if args.print_training_examples:
        print("Training examples:")
        cnt = 16
        for batch in train_loader:
            print(
                "Input:",
                tokenizer.decode(
                    batch["input_ids"][0].tolist(), skip_special_tokens=False
                ),
            )
            print(
                "Target:",
                tokenizer.decode(
                    batch["labels"][0].tolist(), skip_special_tokens=False
                ),
            )
            cnt -= 1
            if cnt == 0:
                break

    if compile:
        model = torch.compile(model)

    trainable_param_names = [
        name for name, param in model.named_parameters() if param.requires_grad
    ]

    use_fsdp = False

    if get_data_parallel_world_size() > 1:
        use_fsdp = True
        model = create_fsdp_model_for_finetune(args, model)
        print("Using FSDP ...")
        print(model)

    optimizer = create_optimizer(
        args,
        model=model,
        optimizer_cpu_offload=args.optimizer_cpu_offload,
        model_cpu_offload=False,
    )

    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        warmup_epochs=len(train_loader) * args.warmup_ratio,
        max_epochs=len(train_loader) * args.num_train_epochs,
        warmup_start_ratio=0.0,
        eta_min_ratio=args.lr_eta_min / args.learning_rate,
    )

    resume_from_checkpoint = None
    resume_epoch = 0
    resume_global_step = 0

    if args.resume_from_checkpoint:
      (
            resume_from_checkpoint,
            resume_epoch,
            resume_global_step,
        ) = get_latest_checkpoint_path(args.save_dir)

        if resume_from_checkpoint is not None:
            print(
                f"Resuming from checkpoint: {resume_from_checkpoint}(epoch {resume_epoch}, global step {resume_global_step})"
            )
            load_checkpoint(
                resume_from_checkpoint, model, optimizer, scheduler, use_fsdp=use_fsdp
            )

    micro_train_batch_size =(
        args.micro_train_batch_size or args.per_device_train_batch_size
    )

    assert(
        args.per_device_train_batch_size % micro_train_batch_size == 0
    ), f"per_device_train_batch_size({args.per_device_train_batch_size}) must be divisible by micro_train_batch_size({micro_train_batch_size})"
    accumulate_steps = args.per_device_train_batch_size // micro_train_batch_size

    print(
        "Batch size per GPU for training: {}\n".format(
            args.per_device_train_batch_size
        ),
        "Micro batch size for training: {}\n".format(micro_train_batch_size),
        "Gradient accumulation steps: {}\n".format(accumulate_steps),
    )

    micro_train_batch_size = micro_train_batch_size * torch.distributed.get_world_size()

    epoch_length = len(train_loader)

    if args.do_train:
        print("Starting training ...")
        t0 = time.time()
        for epoch in tqdm.trange(
            args.num_train_epochs, desc="Epoch", disable=device_id != 0
        ):
            if sampler is not None:
                train_loader.sampler.set_epoch(epoch)
            pbar = tqdm.tqdm(
                enumerate(train_loader),
                desc="Iteration",
                disable=device_id != 0,
                total=len(train_loader),
            )
            for it, batch in pbar:
                global_step = epoch * epoch_length + it
                if global_step < resume_global_step:
                    continue

                torch.cuda.synchronize()
                model.zero_grad()

                input_ids = batch["input_ids"].to(device=device)
                labels = batch["labels"].to(device=device)

                input_ids, labels = prepare_batch(
                    input_ids,
                    labels,
                    tokenizer=tokenizer,
                    use_tp=use_tp,
                    sync_group=intra_node_group,
                )

                loss_scale = 1.0 / accumulate_steps
                for ex_idx in range(0, input_ids.size(0), micro_train_batch_size):
                    if ex_idx + micro_train_batch_size < input_ids.size(0):
                        with torch.cuda.amp.autocast(dtype=args.compute_dtype):
                            loss = model_forward_with_loss(
                                model,
                                input_ids[ex_idx : ex_idx + micro_train_batch_size],
                                labels[ex_idx : ex_idx + micro_train_batch_size],
                            )
                      (loss_scale * loss).backward()
                    else:
                        with torch.cuda.amp.autocast(dtype=args.compute_dtype):
                            loss = model_forward_with_loss(
                                model,
                                input_ids[ex_idx:],
                                labels[ex_idx:],
                            )
                      (loss_scale * loss).backward()
                        grad_norm = clip_grad_norm_(model, 1.0)
                        optimizer.step()
                        scheduler.step()

                if it % 5 == 0:
                    loss_copy = loss.detach().clone()
                    torch.distributed.all_reduce(loss_copy)
                    avg_loss =(loss_copy / torch.distributed.get_world_size()).item()
                    grad_norm_copy = grad_norm.detach().clone().item()

                    if device_id == 0:
                        if args.report_to == "wandb" and wandb is not None:
                            wandb.log(
                                {
                                    "loss": avg_loss,
                                    "learning_rate": scheduler.get_last_lr()[0],
                                    "epoch": epoch,
                                    "step": it,
                                    "grad_norm": grad_norm_copy,
                                },
                                step=global_step,
                            )
                        else:
                            # Just print to stdout.
                            print(
                                {
                                    "loss": avg_loss,
                                    "learning_rate": scheduler.get_last_lr()[0],
                                    "epoch": epoch,
                                    "step": it,
                                    "grad_norm": grad_norm_copy,
                                }
                            )

                checkpoint_hook(
                    args,
                    model,
                    optimizer,
                    scheduler,
                    epoch,
                    global_step,
                    epoch_length,
                    use_fsdp=use_fsdp,
                    trainable_param_names=trainable_param_names,
                )

        torch.cuda.synchronize()

        epoch = args.num_train_epochs

        checkpoint_hook(
            args,
            model,
            optimizer,
            scheduler,
            epoch,
            epoch * epoch_length,
            epoch_length,
            use_fsdp=use_fsdp,
            trainable_param_names=trainable_param_names,
        )

        print(f"Time to train: {time.time() - t0:.02f} seconds")


def prepare_batch(input_ids, labels, tokenizer, use_tp, sync_group):
    pad_id = tokenizer.pad_id
    unk_id = tokenizer.unk_id
    # if pad_id < 0, replace pad_id with unk_id
    labels[labels == pad_id] = IGNORE_INDEX
    if pad_id < 0:
        input_ids[input_ids == pad_id] = unk_id

    if use_tp and get_model_parallel_world_size() > 1:
        # aggregate(concat) all the inputs across tp sync_group
        new_input_ids = torch.empty_like(input_ids).repeat(sync_group.size(), 1)
        new_labels = torch.empty_like(labels).repeat(sync_group.size(), 1)

        torch.distributed.all_gather_into_tensor(
            new_input_ids, input_ids, group=sync_group
        )
        torch.distributed.all_gather_into_tensor(new_labels, labels, group=sync_group)

        return new_input_ids, new_labels

    return input_ids, labels


if __name__ == "__main__":
    parser = HfArgumentParser((TrainingArguments,))
    args = parser.parse_args_into_dataclasses()[0]
    main(args)

```

工程化还证明了 training sequence length 的重要性, 纺织 cufoff, 训练长 thinking 就短:

Training sequence length ablation. We report “accuracy / average thinking tokens per sample”; the higher the accuracy and the fewer the thinking tokens(inference cost) the better.

|                           | Model A       | Model B      |
| :------------------------ | :------------ | :----------- |
| Training sequence length  | 4096          | 32768        |
| % training samples cutoff | 74%           | 0%           |
| AIME24                    | 30.0% / 20721 | 50.0% / 6984 |
| MATH500                   | 90.0% / 5324  | 91.0% / 3268 |
| GPQA                      | 52.5% / 6841  | 53.0% / 3568 |

(直观的 modelA/B 的对比 table 看论文 20 页)

Besides our scaling ablations in §5.2, the main training hyperparameter we ablate is the sequence length used during training.(主要的训练超参数消融研究是训练期间使用的序列长度) We find that a shorter training sequence length leads to longer reasoning traces at test time.(较短的训练序列长度会导致测试时更长的推理轨迹) This is because when training with a shorter sequence length the answer section of the training sample is more commonly cut off.(这是因为使用较短序列长度训练时，训练样本的答案部分更常被截断) Inversely, when the training sequence length is longer, more samples appear in their entirety with the section where the model answers. Thus the model receives more gradient updates where it learns to generate an answer following its chain.(相反，使用较长的训练序列长度时，更多样本会完整地出现在训练过程中，包括模型作答的部分。因此，模型会接收到更多的梯度更新，学习在生成答案时遵循其推理链) This in turn leads to a higher log probability of the answer section at any point during the generation and thus shorter reasoning traces at test time. Performance-wise, we also find that the model trained with a longer sequence length performs better. Thus we opt for the longest training sequence length as it leads to better performance and makes inference more efficient by leading to shorter reasoning traces.

也就是说 DPO 偏好数据训练优化 不是啥好方法, 不应该和 SFT 一起用?

stf.py:

```py
import os
import sys
from dataclasses import dataclass, field, asdict
from typing import Optional
import os
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
from datasets import load_dataset, concatenate_datasets, DatasetDict
import transformers
import trl

@dataclass
class TrainingConfig:
    model_name: str = field(default="Qwen/Qwen2.5-32B-Instruct")
    block_size: int = field(default=32768)
    wandb_project: Optional[str] = field(default="s1")
    wandb_entity: Optional[str] = field(default="hashimoto-group")
    train_file_path: Optional[str] = field(default='simplescaling/s1K_tokenized')
    dagger: bool = field(default=False)

    def __post_init__(self):
        os.environ['WANDB_PROJECT'] = self.wandb_project
        os.environ['WANDB_ENTITY'] = self.wandb_entity

def train():
    # parsing input
    parser = transformers.HfArgumentParser((TrainingConfig, trl.SFTConfig))
    config, args = parser.parse_args_into_dataclasses()
    log_config = {**asdict(config), **asdict(args)}
    logging.info(f"Training config: {log_config}")

    # loading model
    kwargs = {}
    if "70B" in config.model_name:
        # Removed "low_cpu_mem_usage": True, for 70B, since by default we are in FSDP,
        # it's more efficient to do  "cpu_ram_efficient_loading": true, in fsdp_config.json
        kwargs = {"device_map": "auto", "torch_dtype": "auto",
                  "attn_implementation": "flash_attention_2", "use_cache": False}
        model = transformers.AutoModelForCausalLM.from_pretrained(config.model_name, **kwargs)
    else:
        model = transformers.AutoModelForCausalLM.from_pretrained(config.model_name)

    dataset = load_dataset(config.train_file_path)

    # setting up trainer
    tokenizer = transformers.AutoTokenizer.from_pretrained(config.model_name, use_fast=True)
    if "Llama" in config.model_name:
        instruction_template = "<|start_header_id|>user<|end_header_id|>"
        response_template = "<|start_header_id|>assistant<|end_header_id|>\n\n"
        # Use a token that is never used
        tokenizer.pad_token = "<|reserved_special_token_5|>"
    elif "Qwen" in config.model_name:
        instruction_template = "<|im_start|>user"
        response_template = "<|im_start|>assistant\n"
        # Use a token that is never used
        tokenizer.pad_token = "<|fim_pad|>"

    # Only compute loss over assistant responses
    # Verified that it precisely starts where the thinking tokens start and ends with the first pad token
    # via labels being set to -100
    collator = trl.DataCollatorForCompletionOnlyLM(
        instruction_template=instruction_template,
        response_template=response_template,
        tokenizer=tokenizer,
        mlm=False
    )
    args.dataset_text_field = 'text'
    args.max_seq_length = config.block_size
    trainer = trl.SFTTrainer(
        model,
        train_dataset=dataset['train'],
        eval_dataset=dataset['test'] if 'test' in dataset else dataset['train'],
        args=args,
        data_collator=collator
    )

    trainer.train()
    trainer.save_model(output_dir=args.output_dir)
    tokenizer.save_pretrained(args.output_dir)
    trainer.accelerator.wait_for_everyone()


if __name__ == "__main__":
    train()

```

定义训练配置(TrainingConfig)

使用@dataclass 装饰器定义了一个 TrainingConfig 类，用于存储微调过程所需的配置参数，包括模型名称、块大小、WandB(Weights & Biases)项目名称和实体名称、训练文件路径、是否使用 Dagger 等。
在`post_init`方法中，将配置中的 wandb_project 和 wandb_entity 设置为环境变量，以便在训练过程中与其他库或工具集成。

定义训练函数(train)

使用 HfArgumentParser 解析命令行参数，将参数解析为 TrainingConfig 和 trl.SFTConfig 两个数据类对象。
将解析出的配置参数合并为一个字典，并记录到日志中。
根据配置中的模型名称加载预训练模型。如果模型名称包含"70B"，则使用特定的参数来提高大模型的加载效率。
使用 load_dataset 函数从配置中指定的路径加载训练数据集。
根据模型名称设置特定的指令模板(instruction_template)和响应模板(response_template)，并为 tokenizer 设置填充符(pad_token)。
创建一个 DataCollatorForCompletionOnlyLM 对象，用于在训练过程中准备数据批次。只有在助手(assistant)响应的部分计算损失(loss)，并且通过设置标签(labels)为-100 来忽略其他部分。
创建 SFTTrainer 对象，用于微调训练过程。传入模型、训练数据集、评估数据集、微调参数和数据整理器(collator)。
调用 trainer.train()开始微调训练。
训练完成后，将微调后的模型和分词器(tokenizer)保存到指定的输出目录

```py
    # setting up trainer
    tokenizer = transformers.AutoTokenizer.from_pretrained(config.model_name, use_fast=True)
    if "Llama" in config.model_name:
        instruction_template = "<|start_header_id|>user<|end_header_id|>"
        response_template = "<|start_header_id|>assistant<|end_header_id|>\n\n"
        # Use a token that is never used
        tokenizer.pad_token = "<|reserved_special_token_5|>"
    elif "Qwen" in config.model_name:
        instruction_template = "<|im_start|>user"
        response_template = "<|im_start|>assistant\n"
        # Use a token that is never used
        tokenizer.pad_token = "<|fim_pad|>"
```

这里是因为 pad_token 不会与训练数据中的任何实际文本冲突。不同的模型可能使用不同的特殊标记符(special tokens)，因此为了确保填充符不与这些标记符冲突

```sh
uid="$(date +%Y%m%d_%H%M%S)"
base_model="Qwen/Qwen2.5-32B-Instruct"
lr=1e-5
min_lr=0
epochs=5
micro_batch_size=2 # -> batch_size will be 16 if 8 gpus
push_to_hub=true
gradient_accumulation_steps=2
max_steps=-1
gpu_count=$(nvidia-smi -L | wc -l)

torchrun --nproc-per-node ${gpu_count} --master_port 12345 \
train/sft.py \
--per_device_train_batch_size=${micro_batch_size} \
--per_device_eval_batch_size=${micro_batch_size} \
--gradient_accumulation_steps=${gradient_accumulation_steps} \
--num_train_epochs=${epochs} \
--max_steps=${max_steps} \
--train_file_path="simplescaling/s1K_tokenized" \
--model_name=${base_model} \
--warmup_ratio=0.05 \
--fsdp="full_shard auto_wrap" \
--fsdp_config="train/fsdp_config_qwen.json" \
--bf16=True \
--eval_strategy="steps" \
--eval_steps=50 \
--logging_steps=1 \
--save_steps=100 \
--lr_scheduler_type="cosine" \
--learning_rate=${lr} \
--weight_decay=1e-4 \
--adam_beta1=0.9 \
--adam_beta2=0.95 \
--output_dir="ckpts/s1_${uid}" \
--hub_model_id="simplescaling/s1-${uid}" \
--push_to_hub=${push_to_hub} \
--save_only_model=True
```

这里要复习下 fsdp:

FSDP，即全切片数据并行，是一种将数据并行策略推向极致的技术。 与传统的数据并行(DDP)不同，FSDP 不仅将数据集切分为多个分片给不同的 GPU 进行训练，还将模型的参数、优化器状态和梯度都进行了分片。 这样，每个 GPU 只需保存模型的一部分参数，从而显著降低了单个 GPU 的内存占用，使得训练更大规模的模型成为可能。

fsdp：一个布尔值或字符串或关于 FSDPOption 的列表，指定使用 PyTorch Distributed Parallel Training 训练(仅在分布式训练中)。可选的参数为：

"full_shard"：对 parameters, gradients, optimizer states 进行分片。
"shard_grad_op"：对 optimizer states, gradients 进行分片。
"offload"：将 parameters, gradients 卸载到 CPU(仅与 "full_shard", "shard_grad_op" 兼容)。
"auto_wrap"：使用 default_auto_wrap_policy 从而利用 FSDP 来自动递归地 wrap layers 。
fsdp_min_num_params：一个整数，指定 FSDP 的默认的最少的 parameters 数量从而用于 Default Auto Wrapping 。仅当 fsdp 参数有效时 fsdp_min_num_params 才有意义。

@see: https://www.huaxiaozhuan.com/%E5%B7%A5%E5%85%B7/huggingface_transformer/chapters/4_trainer.html

fp16 和 bf16 区别:

FP16 可表示十进制下的三位有效数字格式(log21110∼3.311) ，BF16 可表示十进制下的 2 位有效数字格式(log2810∼2.408)，但按照浮点数的格式，其表示的绝对精度(精确到哪一数位)在各个区间(本质为各个阶码下并不相同)。

BF16 是对 FP32 单精度浮点数截断数据，即用 8bit 表示指数，7bit 表示小数

FP16 半精度浮点数，用 5bit 表示指数，10bit 表示小数\

与 32 位相比，采用 BF16/FP16 吞吐量可以翻倍，内存需求可以减半。但是这两者精度上差异不一样，BF16 可表示的整数范围更广泛，但是尾数精度较小；FP16 表示整数范围较小，但是尾数精度较高。

@see: https://zhuanlan.zhihu.com/p/449345588

因为 s1 用的是 2 个别人的模型 api 做数据筛选工作, 主营业务是 SFT 和推理, 所以用的 BF16?

但为啥不用 FP8 呢? 已经证明了 FP8 内存占用少于 BF16 42%, 速度快 64%

@see: https://www.jiqizhixin.com/articles/2023-11-02-10

@see: https://arxiv.org/abs/2310.18313

@see: https://github.com/Azure/MS-AMP

无法解释
